<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="ngme2">
<title>SPDE approach with non-Gaussian noise • ngme2</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><link href="../deps/Roboto_Slab-0.4.3/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="SPDE approach with non-Gaussian noise">
<meta property="og:description" content="ngme2">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">ngme2</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/ngme2.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Functions</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-vignettes">Vignettes</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-vignettes">
    <a class="dropdown-item" href="../articles/ngme_models.html">Ngme2 Models</a>
    <a class="dropdown-item" href="../articles/Installation_and_configuration.html">Installation and configuration</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/davidbolin/ngme2" aria-label="Github">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://twitter.com/jdavidbolin" aria-label="Twitter">
    <span class="fa fa-twitter"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>SPDE approach with non-Gaussian noise</h1>
            
      
      
      <div class="d-none name"><code>SPDE-approach.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="gaussian-process-in-geostatistics">Gaussian process in geostatistics<a class="anchor" aria-label="anchor" href="#gaussian-process-in-geostatistics"></a>
</h2>
<p>Gaussian process and random fields have a long history, covering
different methods for representing spatial and spatial-temporal
dependence structures. Gaussian fields (GF) have a dominant role in
spatial statistics and especially in the traditional field of
geostatistics.</p>
<p>A common geostatistical model is given by <span class="math display">\[ Y_i = x(\mathbf{s}_i) + \varepsilon_i, \quad
i=1,\ldots,N, \quad \varepsilon_i\sim N(0, \sigma^2),\]</span> <span class="math display">\[x(\mathbf{s}) \sim GP\left(\sum_{k=1}^{n_b}
b_k(\mathbf{s})w_k, c(\mathbf{s},\mathbf{s}')\right),\]</span> where
<span class="math inline">\(N\)</span> is the number of spatial
observations, <span class="math inline">\(GP(m,c)\)</span> stands for a
Gaussian process with mean function <span class="math inline">\(m\)</span> and covariance function <span class="math inline">\(c\)</span>, <span class="math inline">\(n_b\)</span> is the number of basis functions,
<span class="math inline">\(\{b_k(\cdot)\}_{k=1}^{n_b}\)</span> are
basis functions, <span class="math inline">\(w_k\)</span> are weights to
be estimated and <span class="math inline">\(c(\cdot,\cdot)\)</span> is
a covariance function.</p>
<p>A popular and flexible covariance function for random fields on <span class="math inline">\(\mathbb{R}^d\)</span> is the Matérn covariance
function:</p>
<p><span class="math display">\[
c(\mathbf{s}, \mathbf{s}') =
\frac{\sigma^2}{\Gamma(\nu)2^{\nu-1}}(\kappa
\|\mathbf{s}-\mathbf{s}'\|)^\nu
K_\nu(\kappa\|\mathbf{s}-\mathbf{s}'\|),
\]</span></p>
<p>where <span class="math inline">\(\Gamma(\cdot)\)</span> is the Gamma
function, <span class="math inline">\(K_\nu(\cdot)\)</span> is the
modified Bessel function of the second kind, <span class="math inline">\(\nu&gt;0\)</span> controls the correlation range
and <span class="math inline">\(\sigma^2\)</span> is the variance.
Finally, <span class="math inline">\(\nu&gt;0\)</span> determines the
smoothness of the field.</p>
<p>Usually, the model parameters are estimated via maximum likelihood
estimation. The main drawback with this approach is that the
computational time needed in order to perform statistical inference
usually scales as <span class="math inline">\(\mathcal{O}(N^3)\)</span>.</p>
<div class="section level3">
<h3 id="the-spde-approach-with-gaussian-noise">The SPDE approach with Gaussian noise<a class="anchor" aria-label="anchor" href="#the-spde-approach-with-gaussian-noise"></a>
</h3>
<p>It is well-known (Whittle, 1963) that a Gaussian process <span class="math inline">\(u(\mathbf{s})\)</span> with Matérn covariance
function solves the stochastic partial differential equation (SPDE)</p>
<p><span class="math display">\[\begin{equation}\label{spde}
(\kappa^2 -\Delta)^\beta u = \mathcal{W}\quad \hbox{in } \mathcal{D},
\end{equation}\]</span> where <span class="math inline">\(\Delta =
\sum_{i=1}^d \frac{\partial^2}{\partial_{x_i^2}}\)</span> is the
Laplacian operator, <span class="math inline">\(\mathcal{W}\)</span> is
the Gaussian spatial white noise on <span class="math inline">\(\mathcal{D}=\mathbb{R}^d\)</span>, and <span class="math inline">\(4\beta = 2\nu + d\)</span>.</p>
<p>Inspired by this relation between Gaussian processes with Matérn
covariance functions and solutions of the above SPDE, <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2011.00777.x" class="external-link">Lindgren
et al. (2011)</a> constructed computationally efficient Gaussian Markov
random field approximations of <span class="math inline">\(u(\mathbf{s})\)</span>, where the domain <span class="math inline">\(\mathcal{D}\subsetneq \mathbb{R}^d\)</span> is
bounded and <span class="math inline">\(2\beta\in\mathbb{N}\)</span>.
The approximate solutions of the SPDE are obtained through a finite
element discretization.</p>
</div>
<div class="section level3">
<h3 id="finite-element-approximation">Finite element approximation<a class="anchor" aria-label="anchor" href="#finite-element-approximation"></a>
</h3>
<p>We will now provide a brief description of the finite element method
they used. To make the description simpler we will consider the
nonfractional SPDE given by <span class="math display">\[(\kappa^2 -
\Delta) u(\mathbf{s}) = \mathcal{W}(\mathbf{s}),\]</span> on some
bounded domain <span class="math inline">\(\mathcal{D}\)</span> in <span class="math inline">\(\mathbb{R}^d\)</span>. The Laplacian operator is
augmented with boundary conditions. Usually one considers Dirichlet, in
which the process is zero on the boundary of <span class="math inline">\(\mathcal{D}\)</span>, or Neumann, in which the
directional derivarives of the process in the normal directions is zero
on the boundary of <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>The equation is interpreted in the following weak sense: for every
function <span class="math inline">\(\psi(\mathbf{s})\)</span> from some
suitable space of test functions, the following identity holds <span class="math display">\[\langle \psi,
(\kappa^2-\Delta)u\rangle_{\mathcal{D}} \stackrel{d}{=} \langle \psi,
\mathcal{W}\rangle_{\mathcal{D}},\]</span> where <span class="math inline">\(\stackrel{d}{=}\)</span> means equality in
distribution and <span class="math inline">\(\langle\cdot,\cdot\rangle_{\mathcal{D}}\)</span>
is the standard inner product in <span class="math inline">\(L_2(\mathcal{D})\)</span>, <span class="math inline">\(\langle f,g\rangle_{\mathcal{D}} =
\int_\mathcal{D} f(\mathbf{s})g(\mathbf{s}) d\mathbf{s}.\)</span></p>
<p>The finite element method (FEM) consists on considering a finite
dimensional space of test functions <span class="math inline">\(V_n\)</span>. In the Galerkin method, we consider
<span class="math inline">\(V_n = {\rm
span}\{\varphi_1,\ldots,\varphi_n\}\)</span>, where <span class="math inline">\(\varphi_i(\mathbf{s}), i=1,\ldots, n\)</span> are
piecewise linear basis functions obtained from a triangulation of <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>Then, we write approximate the solution <span class="math inline">\(u\)</span> by <span class="math inline">\(u_n\)</span>, where <span class="math inline">\(u_n\)</span> is written in terms of the basis
functions as <span class="math display">\[u_n(\mathbf{s}) = \sum_{i=1}^n
w_i \varphi_i(\mathbf{s}).\]</span></p>
<p>We thus obtain the system of linear equations <span class="math display">\[\left\langle \varphi_j, (\kappa^2 -
\Delta)\left(\sum_{i=1}^n w_i\varphi_i\right)\right\rangle_{\mathcal{D}}
\stackrel{d}{=} \langle \varphi_j,
\mathcal{W}\rangle_{\mathcal{D}},\quad\hbox{for }
j=1,\ldots,n.\]</span></p>
<p>The right hand side can be shown that <span class="math display">\[(\langle \varphi_1,
\mathcal{W}\rangle_{\mathcal{D}}, \ldots, \langle \varphi_n,
\mathcal{W}\rangle_{\mathcal{D}}) \sim N(0, \mathbf{C}),\]</span> where
<span class="math inline">\(\mathbf{C}\)</span> is an <span class="math inline">\(n\times n\)</span> matrix with <span class="math inline">\((i,j)\)</span>th entry given by <span class="math display">\[\mathbf{C}_{i,j} = \int_{\mathcal{D}}
\varphi_i(\mathbf{s})\varphi_j(\mathbf{s}) d\mathbf{s}.\]</span> The
matrix <span class="math inline">\(\mathbf{C}\)</span> is known as the
<em>mass matrix</em> in FEM theory.</p>
<p>By using Green’s first identity, the left hand side is <span class="math display">\[
\begin{array}{ccl}
\left\langle \varphi_j, (\kappa^2 - \Delta)\left(\sum_{i=1}^n
w_i\varphi_i\right)\right\rangle_{\mathcal{D}} &amp;=&amp; \sum_{i=1}^n
\langle \varphi_j, (\kappa^2 -
\Delta)w_i\varphi_i\rangle_{\mathcal{D}}\\
&amp;=&amp; \sum_{i=1}^n (\kappa^2 \langle \varphi_j,
\varphi_i\rangle_{\mathcal{D}} + \langle \nabla \varphi_j, \nabla
\varphi_i\rangle_{\mathcal{D}}) w_i, \quad j=1,\ldots, n,
\end{array}
\]</span> where the boundary terms vanish due to boundary conditions
(for both Dirichlet and Neumann). We can then rewrite the last term in
matrix form as <span class="math display">\[(\kappa^2 \mathbf{C} +
\mathbf{G})\mathbf{w},\]</span> where <span class="math inline">\(\mathbf{w} = (w_1,\ldots,w_n)\)</span> and <span class="math inline">\(\mathbf{G}\)</span> is an <span class="math inline">\(n\times n\)</span> matrix with <span class="math inline">\((i,j)\)</span>th entry given by <span class="math display">\[\mathbf{G}_{i,j} = \int_{\mathcal{D}} \nabla
\varphi_i(\mathbf{s})\nabla\varphi_j(\mathbf{s})d\mathbf{s}.\]</span>
The matrix <span class="math inline">\(\mathbf{G}\)</span> is known in
FEM theory as stiffness matrix.</p>
<p>Putting everything together, we have that <span class="math display">\[(\kappa^2 \mathbf{C} + \mathbf{G}) \mathbf{w}
\sim N(0,\mathbf{C}).\]</span> Therefore, <span class="math inline">\(\mathbf{w}\)</span> is a centered Gaussian
variable with precision matrix given by <span class="math display">\[\mathbf{Q} = (\kappa^2
\mathbf{C}+\mathbf{G})^\top \mathbf{C}^{-1}(\kappa^2
\mathbf{C}+\mathbf{G}).\]</span></p>
</div>
<div class="section level3">
<h3 id="computational-advantages-of-the-spde-approach">Computational advantages of the SPDE approach<a class="anchor" aria-label="anchor" href="#computational-advantages-of-the-spde-approach"></a>
</h3>
<p>For spatial problems, the computational cost usually scales as <span class="math inline">\(\mathcal{O}(n^{3/2})\)</span>, where <span class="math inline">\(n\)</span> is the number of basis functions. This
should be compared to the <span class="math inline">\(\mathcal{O}(N^3)\)</span> of the Gaussian random
field approach.</p>
<p>This implies in accurate approximations which drastically reduces the
computational cost for sampling and inference.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-spde-approach-with-non-gaussian-noise">The SPDE approach with non-Gaussian noise<a class="anchor" aria-label="anchor" href="#the-spde-approach-with-non-gaussian-noise"></a>
</h2>
<p>Then we will describe how to generalize this approach with
non-Gaussian noise. Our goal now is to describe the SPDE approach when
the noise is non-Gaussian. The motivation for handling non-Gaussian
noise comes from the fact that many features cannot not be handled by
Gaussian noise. Some of these reasons are:</p>
<ul>
<li>Skewness;</li>
<li>Heavier tails;</li>
<li>Jumps in the sample paths;</li>
<li>Asymmetries in the sample paths.</li>
</ul>
<div class="section level3">
<h3 id="non-gaussian-matérn-fields">Non-Gaussian Matérn fields<a class="anchor" aria-label="anchor" href="#non-gaussian-mat%C3%A9rn-fields"></a>
</h3>
<p>The idea is to replace the Gaussian white noise <span class="math inline">\(\mathcal{W}\)</span> in the SPDE by a non-Gaussian
white noise <span class="math inline">\(\dot{\mathcal{M}}\)</span>:
<span class="math display">\[(\kappa^2 - \Delta)^\beta u =
\dot{\mathcal{M}}.\]</span> The solution <span class="math inline">\(u\)</span> will have Matérn covariance function,
but their marginal distributions will be non-Gaussian.</p>
<p>We will consider the same setup. More precisely, we consider <span class="math inline">\(V_n = {\rm
span}\{\varphi_1,\ldots,\varphi_n\}\)</span>, where <span class="math inline">\(\varphi_i(\mathbf{s}), i=1,\ldots, n\)</span> are
piecewise linear basis functions obtained from a triangulation of <span class="math inline">\(\mathcal{D}\)</span> and we approximate the
solution <span class="math inline">\(u\)</span> by <span class="math inline">\(u_n\)</span>, where <span class="math inline">\(u_n\)</span> is written in terms of the basis
functions as <span class="math display">\[u_n(\mathbf{s}) = \sum_{i=1}^n
w_i \varphi_i(\mathbf{s}).\]</span> In the right-hand side we obtain a
random vector <span class="math display">\[\mathbf{f} =
(\dot{\mathcal{M}}(\varphi_1),\ldots,
\dot{\mathcal{M}}(\varphi_n)),\]</span> where the functional <span class="math inline">\(\dot{\mathcal{M}}\)</span> is given by <span class="math display">\[\dot{\mathcal{M}}(\varphi_j) = \int_{\mathcal{D}}
\varphi_j(\mathbf{s}) d\mathcal{M}(\mathbf{s}).\]</span> By considering
<span class="math inline">\(\mathcal{M}\)</span> to be a type-G Lévy
process, we obtain that <span class="math inline">\(\mathbf{f}\)</span>
has a joint distribution that is easy to handle.</p>
<p>We say that a Lévy process is of type G if its increments can be
represented as location-scale mixtures: <span class="math display">\[\gamma + \mu V + \sigma \sqrt{V}Z,\]</span> where
<span class="math inline">\(\gamma, \mu\)</span> are parameters, <span class="math inline">\(Z\sim N(0,1)\)</span> and is independent of <span class="math inline">\(V\)</span>, and <span class="math inline">\(V\)</span> is a positive infinitely divisible
random variable.</p>
<p>Therefore, given a vector <span class="math inline">\(\mathbf{V} =
(V_1,\ldots,V_n)\)</span> of independent stochastic variances (in our
case, positive infinitely divisible random variables), we obtain that
<span class="math display">\[\mathbf{f}|\mathbf{V} \sim N(\gamma +
\mu\mathbf{V}, \sigma^2{\rm diag}(\mathbf{V})).\]</span> So, if we
consider, for instance, the non-fractional and non-Gaussian SPDE <span class="math display">\[(\kappa^2 - \Delta) u =
\dot{\mathcal{M}},\]</span> we obtain that the FEM weights <span class="math inline">\(\mathbf{w} = (w_1,\ldots,w_n)\)</span> satisfy
<span class="math display">\[\mathbf{w}|\mathbf{V} \sim
N(\mathbf{K}^{-1}(\gamma+\mu\mathbf{V}), \sigma^2\mathbf{K}^{-1}{\rm
diag}(\mathbf{V})\mathbf{K}^{-1}),\]</span> where <span class="math inline">\(\mathbf{K} =
\kappa^2\mathbf{C}+\mathbf{G}\)</span> is the discretization of the
differential operator.</p>
</div>
<div class="section level3">
<h3 id="the-nig-model">The NIG model<a class="anchor" aria-label="anchor" href="#the-nig-model"></a>
</h3>
<p>We will delve into more details now by considering, as example, the
NIG model.</p>
<p>First, we say that a random variable <span class="math inline">\(V\)</span> follows an inverse Gaussian
distribution with parameters <span class="math inline">\(\eta_1\)</span>
and <span class="math inline">\(\eta_2\)</span>, denoted by <span class="math inline">\(V\sim IG(\eta_1,\eta_2)\)</span> if it has
probability density function (pdf) given by <span class="math display">\[\pi(v) = \frac{\sqrt{\eta_2}}{\sqrt{2\pi v^3}}
\exp\left\{-\frac{\eta_1}{2}v - \frac{\eta_2}{2v} +
\sqrt{\eta_1\eta_2}\right\},\quad \eta_1,\eta_2&gt;0.\]</span> We can
generate samples of inverse Gaussian distributions with parameters <span class="math inline">\(\eta_1\)</span> and <span class="math inline">\(\eta_2\)</span> by generating samples from the <a href="https://en.wikipedia.org/wiki/Generalized_inverse_Gaussian_distribution" class="external-link">generalized
inverse Gaussian distribution</a> with parameters <span class="math inline">\(p=-1/2\)</span>, <span class="math inline">\(a=\eta_1\)</span> and <span class="math inline">\(b=\eta_2\)</span>. We can use the <em>rGIG</em>
function to generate samples from the generalized inverse Gaussian
distribution.</p>
<p>If <span class="math inline">\(V\sim IG(\eta_1,\eta_2)\)</span>, then
<span class="math inline">\(X = \gamma +\mu V + \sigma
\sqrt{V}Z\)</span>, with <span class="math inline">\(Z\sim
N(0,1)\)</span>, being independent of <span class="math inline">\(V\)</span>, then <span class="math inline">\(X\)</span> follows a normal inverse Gaussian (NIG)
distribution and has pdf <span class="math display">\[\pi(x) =
\frac{e^{\sqrt{\eta_1\eta_2}+\mu(x-\gamma)/\sigma^2}\sqrt{\eta_2\mu^2/\sigma^2+\eta_1\eta_2}}{\pi\sqrt{\eta_2\sigma^2+(x-\gamma)^2}}
K_1\left(\sqrt{(\eta_2\sigma^2+(x-\gamma)^2)(\mu^2/\sigma^4+\eta_1/\sigma^2)}\right),\]</span>
where <span class="math inline">\(K_1\)</span> is a modified Bessel
function of the third kind. In this form, the NIG density is
overparameterized, and we therefore set <span class="math inline">\(\eta_1=\eta_2=\eta\)</span>, which results in
<span class="math inline">\(E(V)=1\)</span>. Thus, one have the
parameters, <span class="math inline">\(\mu, \gamma\)</span> and <span class="math inline">\(\eta\)</span>.</p>
<p>The NIG model thus assumes that the stochastic variance <span class="math inline">\(V_i\)</span> follows an inverse Gaussian with
parameters <span class="math inline">\(\eta\)</span> and <span class="math inline">\(\eta h_i^2\)</span>, where <span class="math inline">\(h_i = \int_{\mathcal{D}} \varphi_i(\mathbf{s})
d\mathbf{s}.\)</span></p>
<p><img src="SPDE-approach_files/figure-html/nig-1.png" width="672"></p>
</div>
<div class="section level3">
<h3 id="prediction">Prediction<a class="anchor" aria-label="anchor" href="#prediction"></a>
</h3>
<p>Our goal in this section is to perform prediction of the latent field
<span class="math inline">\(u\)</span> at locations where there are no
observations. Usually, when doing such predictions, one provides mean
and variance of the predictive distribution.</p>
<p>Let us assume we want to obtain predictions at locations <span class="math inline">\(\widetilde{\mathbf{s}}_1, \ldots,
\widetilde{\mathbf{s}}_p \in \mathcal{D}\)</span>, where <span class="math inline">\(p\in \mathbb{N}\)</span>.</p>
<p>Notice that for <span class="math inline">\(j=1,\ldots,p\)</span>,
<span class="math display">\[u_n(\widetilde{\mathbf{s}}_j) =
\sum_{i=1}^n w_i \varphi_i(\widetilde{\mathbf{s}}_j).\]</span></p>
<p>Therefore, if we let <span class="math inline">\(\mathbf{A}_p\)</span> be the <span class="math inline">\(p\times n\)</span> matrix whose <span class="math inline">\((i,j)\)</span>th entry is given by <span class="math inline">\(\mathbf{A}_{p,ij} =
\varphi_j(\widetilde{\mathbf{s}}_i)\)</span>, then <span class="math display">\[(u_n(\widetilde{\mathbf{s}}_1),\ldots,
u_n(\widetilde{\mathbf{s}}_p)) = \mathbf{A}_p\mathbf{w}.\]</span> Thus,
to perform prediction the desired means and variances are <span class="math display">\[E[\mathbf{A}_p \mathbf{w}  |
\mathbf{Y}]\quad\hbox{and}\quad
V[\mathbf{A}_p\mathbf{w}|\mathbf{Y}],\]</span> where <span class="math inline">\(\mathbf{Y} = (Y_1,\ldots,Y_N).\)</span></p>
<p>Now, observe that the density of <span class="math inline">\(\mathbf{w}|\mathbf{Y}\)</span> is not known. So,
the mean and variance cannot be computed analytically.</p>
<p>There are two ways to circumvent that situation. Both of them are
based on the fact that even though we do not know the density of <span class="math inline">\(\mathbf{w}|\mathbf{Y}\)</span>, we do know the
density of <span class="math inline">\(\mathbf{V}|\mathbf{w},\mathbf{Y}\)</span> and the
density of <span class="math inline">\(\mathbf{w}|\mathbf{V},\mathbf{Y}\)</span>.
Therefore we can use a Gibbs sampler to sample from <span class="math inline">\((\mathbf{w},\mathbf{V})|\mathbf{Y}\)</span>. From
this we obtain, as a byproduct, marginal samples from <span class="math inline">\(\mathbf{w}|\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{V}|\mathbf{Y}\)</span>.</p>
<p>We will now provide a brief presentation of the Gibbs sampler and
then we will provide the approximations of the means and variances.</p>
</div>
<div class="section level3">
<h3 id="gibbs-sampler">Gibbs sampler<a class="anchor" aria-label="anchor" href="#gibbs-sampler"></a>
</h3>
<p>In this section, we will briefly describe the Gibbs sampler algorithm
we use.</p>
<p>Let <span class="math inline">\(\mathbf{A}\)</span> be the <span class="math inline">\(N\times n\)</span> matrix, whose <span class="math inline">\((i,j)\)</span>th entry is given by <span class="math inline">\(\mathbf{A}_{ij} =
\varphi_j(\mathbf{s}_i)\)</span>. Therefore, we have that <span class="math display">\[(u_n(\mathbf{s}_1),\ldots,u_n(\mathbf{s}_N)) =
\mathbf{A}\mathbf{w},\]</span> so that <span class="math display">\[\mathbf{Y} = \mathbf{A}\mathbf{w} +
\boldsymbol{\varepsilon},\]</span> where <span class="math inline">\(\boldsymbol{\varepsilon} =
(\varepsilon_1,\ldots,\varepsilon_N)\)</span> is the measurement noise.
Here we assumen <span class="math inline">\(\varepsilon\)</span> is
Gaussian noise.</p>
<p>Therefore, under this assumption we have that <span class="math display">\[\mathbf{Y}|\mathbf{w} \sim
N(\mathbf{A}\mathbf{w}, \sigma_\varepsilon^{2} \mathbf{I}).\]</span>
Also recall that <span class="math display">\[\mathbf{w}|\mathbf{V} \sim
N(\mathbf{K}^{-1}(-\mu \mathbf{h}+\mu\mathbf{V}),
\sigma^2\mathbf{K}^{-1}{\rm diag}(\mathbf{V})\mathbf{K}^{-1}).\]</span>
Let <span class="math display">\[\mathbf{m} = \mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V})\quad \hbox{and}\quad \mathbf{Q} =
\frac{1}{\sigma^2}\mathbf{K}{\rm
diag}(\mathbf{V})^{-1}\mathbf{K}.\]</span></p>
<p>It thus follows (see, also, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/sjos.12141" class="external-link">Wallin
and Bolin (2015)</a> or <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssc.12405?af=R" class="external-link">Asar
et al. (2020)</a>) that <span class="math display">\[\mathbf{w} |
\mathbf{V}, \mathbf{Y} \sim N\big(\widetilde{\mathbf{m}},
\widetilde{\mathbf{Q}}^{-1}),\]</span> where <span class="math display">\[\widetilde{\mathbf{Q}} = \mathbf{Q} +
\sigma_\varepsilon^{-2} \mathbf{A}^\top\mathbf{A}\quad\hbox{and}\quad
\widetilde{\mathbf{m}} =
\widetilde{\mathbf{Q}}^{-1}\big(\mathbf{Q}\mathbf{m}+\sigma_\varepsilon^{-2}\mathbf{A}^\top\mathbf{Y}\big).\]</span></p>
<p>To compute the conditional distribution <span class="math inline">\(\mathbf{V}|\mathbf{w}, \mathbf{Y}\)</span> one can
see from <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/sjos.12141" class="external-link">Wallin
and Bolin (2015)</a>, pp. 879, that <span class="math inline">\(V_1,\ldots,V_n\)</span> are conditionally
independent given <span class="math inline">\(\mathbf{w}\)</span>.
Furthermore, we also have from Proposition 1 from <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssc.12405?af=R" class="external-link">Asar
et al. (2020)</a>) that if <span class="math inline">\(V\sim
GIG(p,a,b)\)</span>, where <span class="math inline">\(GIG\)</span>
stands for the <a href="https://en.wikipedia.org/wiki/Generalized_inverse_Gaussian_distribution" class="external-link">generalized
inverse Gaussian distribution</a> with parameters <span class="math inline">\(p, a\)</span> and <span class="math inline">\(b\)</span>, then, for every <span class="math inline">\(j=1,\ldots,n\)</span>, <span class="math display">\[V_j|\mathbf{w},\mathbf{Y} \sim GIG\Bigg(p-0.5,
a+\frac{\mu^2}{\sigma^2}, b +
\frac{(\mathbf{K}\mathbf{w}+\mathbf{h}\mu)_j^2}{\sigma^2}\Bigg).\]</span></p>
<p>We are now in a position to use the Gibbs sampling algorithm:</p>
<ul>
<li>Provide initial values <span class="math inline">\(\mathbf{V}^{(0)}\)</span>;</li>
<li>Sample <span class="math inline">\(\mathbf{w}^{(1)} |
\mathbf{V}^{(0)},\mathbf{Y}\)</span>;</li>
<li>Sample <span class="math inline">\(\mathbf{V}^{(1)} |
\mathbf{w}^{(1)}, \mathbf{Y}\)</span>;</li>
<li>Continue by sequentially sampling <span class="math inline">\(\mathbf{w}^{(i)}|\mathbf{V}^{(i-1)},\mathbf{Y}\)</span>,
and then <span class="math inline">\(\mathbf{V}^{(i)}|\mathbf{w}^{(i)},
\mathbf{Y}\)</span> for <span class="math inline">\(i=1,\ldots,k\)</span>.</li>
</ul>
<p>One should stop when equilibrium is reached. To obtain evidence that
equilibrium has been achieved, it is best to consider more than one
chain, starting from different locations, and see if they mixed well. It
might also be useful to see autocorrelation plots.</p>
<p>Depending on the starting values, one might consider to do
<strong>burn-in</strong> samples, that is, one runs a chain for some
iterations, then saves the last position, throw away the rest of the
samples, and use that as starting values.</p>
<p>It is important to observe that the samples <span class="math inline">\(\{\mathbf{w}^{(i)},\mathbf{V}^{(i)}\}_{i=1}^k\)</span>
will not be independent. However, under very general assumptions, the
Gibbs sampler provides samples satisfying the law of large numbers for
functionals of the sample. Therefore, one can use these samples to
compute means and variances.</p>
</div>
<div class="section level3">
<h3 id="estimation">Estimation<a class="anchor" aria-label="anchor" href="#estimation"></a>
</h3>
<p>In ngme2, we use maximum likelihood estimation via preconditioned
stochastic gradient descent. The gradient is approximated by</p>
<div class="section level4">
<h4 id="standard-mc-approximation-of-the-gradient">Standard MC approximation of the gradient<a class="anchor" aria-label="anchor" href="#standard-mc-approximation-of-the-gradient"></a>
</h4>
<p>In our context, we assume <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> to be hidden. Therefore, we
may use Fisher’s identity (Fisher, 1925) to the latent variable <span class="math inline">\((\mathbf{V},\mathbf{w})\)</span> to obtain that
<span class="math display">\[\nabla_{\boldsymbol{\theta}}
L({\boldsymbol{\theta}}; \mathbf{Y}) =
E_{\mathbf{V},\mathbf{w}}[\nabla_{\boldsymbol{\theta}}
L({\boldsymbol{\theta}}; \mathbf{Y}, \mathbf{V},
\mathbf{w})|\mathbf{Y}].\]</span></p>
<p>Thus, the idea here is to use both samples of <span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> obtained from the Gibbs
sampler to approximate the gradient as <span class="math display">\[\nabla_{{\boldsymbol{\theta}}}L({\boldsymbol{\theta}};\mathbf{Y})
\approx \frac{1}{k} \sum_{j=1}^k \nabla_{{\boldsymbol{\theta}}}
L({\boldsymbol{\theta}};\mathbf{Y},\mathbf{V}^{(j)},
\mathbf{w}^{(j)}).\]</span> To this end, we will compute the gradients
<span class="math inline">\(\nabla_{{\boldsymbol{\theta}}}
L({\boldsymbol{\theta}};\mathbf{Y},\mathbf{V}, \mathbf{w})\)</span>. We
have that <span class="math display">\[\mathbf{Y}|\mathbf{w} \sim
N(\mathbf{A}\mathbf{w}, \sigma_\varepsilon^{-2} \mathbf{I}),\]</span>
<span class="math display">\[\mathbf{w}|\mathbf{V} \sim
N(\mathbf{K}^{-1}(-\mu \mathbf{h}+\mu\mathbf{V}), \sigma^2
\mathbf{K}^{-1}{\rm diag}(\mathbf{V})\mathbf{K}^{-1})\]</span> and <span class="math inline">\(\mathbf{V}\)</span> follows a GIG distribution
such that for every <span class="math inline">\(i\)</span>, <span class="math inline">\(E[V_i]=h_i\)</span>. Therefore, we have that <span class="math display">\[\begin{array}{ccl}
L((\mu,\sigma_\varepsilon); \mathbf{w}, \mathbf{V},\mathbf{Y})
&amp;=&amp; -n\log(\sigma_\varepsilon)-0.5\sigma_\varepsilon^{-2}
(\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V}))\\
&amp;-&amp;0.5\sigma_\varepsilon^{-2}(\mathbf{A}(\mathbf{w}-\mathbf{m})^\top{\rm
diag} (1/(\sigma^2 V_i)) (\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V})-\mathbf{A}(\mathbf{w}-\mathbf{m})) + const,
\end{array}\]</span> where <span class="math inline">\(const\)</span>
does not depend on <span class="math inline">\((\mu,\sigma)\)</span>.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li><p>Lindgren, F., Rue, H., and Lindstrom, J. (2011). An explicit link
between Gaussian fields and Gaussian Markov random fields: the
stochastic partial differential equation approach. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
73(4):423–498.</p></li>
<li><p>Whittle, P. (1963). Stochastic-processes in several dimensions.
Bulletin of the International Statistical Institute,
40(2):974–994.</p></li>
<li><p>Wallin, J., Bollin, D. (2015). Geostatistical Modelling Using
Non-Gaussian Matérn Fields. Scandinavian Journal of Statistics.
42(3):872-890.</p></li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by David Bolin, Xiaotian Jin, Alexandre Simas, Jonas Wallin.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
