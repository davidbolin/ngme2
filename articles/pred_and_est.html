<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Model estimation and prediction • ngme2</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Model estimation and prediction">
<meta property="og:description" content="ngme2">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">ngme2</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.6.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/ngme2.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Vignettes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Installation_and_configuration.html">Installation and configuration</a>
    </li>
    <li>
      <a href="../articles/inla_compare.html">Comparison with R-INLA</a>
    </li>
    <li>
      <a href="../articles/AR1-model.html">AR(1) model</a>
    </li>
    <li>
      <a href="../articles/RW-model.html">Random walk model</a>
    </li>
    <li>
      <a href="../articles/SPDE-model.html">SPDE Matern model</a>
    </li>
    <li>
      <a href="../articles/replicate.html">Ngme replicate feature</a>
    </li>
    <li>
      <a href="../articles/pred_and_est.html">Model estimation and prediction</a>
    </li>
    <li>
      <a href="../articles/bivariate.html">Bivariate models in Ngme2</a>
    </li>
    <li>
      <a href="../articles/random-effect.html">Random effects model in Ngme2</a>
    </li>
    <li>
      <a href="../articles/tensor_product.html">Space-time (tensor product) model in Ngme2</a>
    </li>
    <li>
      <a href="../articles/matern-on-graph.html">Matern SPDE model on Metric Graph</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/davidbolin/ngme2" class="external-link">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/jdavidbolin" class="external-link">
    <span class="fa fa-twitter"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Model estimation and prediction</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/davidbolin/ngme2/blob/HEAD/vignettes/pred_and_est.Rmd" class="external-link"><code>vignettes/pred_and_est.Rmd</code></a></small>
      <div class="hidden name"><code>pred_and_est.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>In this vignette, we provide a breif introduction to the estimation
methods in <code>ngme2</code>. First, we give a description of the
model.</p>
<p>A popular and flexible covariance function for random fields on <span class="math inline">\(\mathbb{R}^d\)</span> is the Matérn covariance
function: <span class="math display">\[c(\mathbf{s}, \mathbf{s}') =
\frac{\sigma^2}{\Gamma(\nu)2^{\nu-1}}(\kappa
\|\mathbf{s}-\mathbf{s}'\|)^\nu
K_\nu(\kappa\|\mathbf{s}-\mathbf{s}'\|),\]</span> where <span class="math inline">\(\Gamma(\cdot)\)</span> is the Gamma function,
<span class="math inline">\(K_\nu(\cdot)\)</span> is the modified Bessel
function of the second kind, <span class="math inline">\(\nu&gt;0\)</span> controls the correlation range
and <span class="math inline">\(\sigma^2\)</span> is the variance.
Finally, <span class="math inline">\(\nu&gt;0\)</span> determines the
smoothness of the field.</p>
<p>It is well-known (Whittle, 1963) that a Gaussian process <span class="math inline">\(u(\mathbf{s})\)</span> with Matérn covariance
function solves the stochastic partial differential equation (SPDE)
<span class="math display">\[\begin{equation}\label{spde}
(\kappa^2 -\Delta)^\beta u = \mathcal{W}\quad \hbox{in } \mathcal{D},
\end{equation}\]</span> where <span class="math inline">\(\Delta =
\sum_{i=1}^d \frac{\partial^2}{\partial_{x_i^2}}\)</span> is the
Laplacian operator, <span class="math inline">\(\mathcal{W}\)</span> is
the Gaussian spatial white noise on <span class="math inline">\(\mathcal{D}=\mathbb{R}^d\)</span>, and <span class="math inline">\(4\beta = 2\nu + d\)</span>.</p>
<p>Inspired by this relation between Gaussian processes with Matérn
covariance functions and solutions of the above SPDE, <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2011.00777.x" class="external-link">Lindgren
et al. (2011)</a> constructed computationally efficient Gaussian Markov
random field approximations of <span class="math inline">\(u(\mathbf{s})\)</span>, where the domain <span class="math inline">\(\mathcal{D}\subsetneq \mathbb{R}^d\)</span> is
bounded and <span class="math inline">\(2\beta\in\mathbb{N}\)</span>.</p>
<p>In order to model departures from Gaussian behaviour we will consider
the following extension due to <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/sjos.12046" class="external-link">Bolin
(2014)</a>: <span class="math display">\[(\kappa^2 - \Delta)^\beta
X(\mathbf{s}) = \dot{\mathcal{M}}(\mathbf{s}),\quad
\mathbf{s}\in\mathcal{D},\]</span> where <span class="math inline">\(\dot{\mathcal{M}}\)</span> is a non-Gaussian
white-noise. More specifically, we assume <span class="math inline">\(\mathcal{M}\)</span> to be a type-G Lévy
process.</p>
<p>We say that a Lévy process is of type G if its increments can be
represented as location-scale mixtures: <span class="math display">\[\gamma + \mu V + \sigma \sqrt{V}Z,\]</span> where
<span class="math inline">\(\gamma, \mu\)</span> and <span class="math inline">\(\sigma\)</span> are parameters, <span class="math inline">\(Z\sim N(0,1)\)</span> and is independent of <span class="math inline">\(V\)</span>, and <span class="math inline">\(V\)</span> is a positive infinitely divisible
random variable. In the SPDE we will assume <span class="math inline">\(\gamma = -\mu E(V)\)</span> and <span class="math inline">\(\sigma = 1\)</span>.</p>
<p>Finally, we assume we have observations <span class="math inline">\(Y_1,\ldots,Y_N\)</span>, observed at locations
<span class="math inline">\(\mathbf{s}_1,\ldots,\mathbf{s}_N\in\mathcal{D}\)</span>,
where <span class="math inline">\(Y_1,\ldots, Y_N\)</span> satisfy <span class="math display">\[ Y_i = X(\mathbf{s}_i) + \varepsilon_i, \quad
i=1,\ldots,N,\]</span> where <span class="math inline">\(\varepsilon_1,\ldots,\varepsilon_N\)</span> are
i.i.d. following <span class="math inline">\(\varepsilon_i\sim N(0,
\sigma_\varepsilon^2)\)</span>.</p>
</div>
<div class="section level2">
<h2 id="finite-element-approximations">Finite element approximations<a class="anchor" aria-label="anchor" href="#finite-element-approximations"></a>
</h2>
<p>In this vignette we will assume basic understanding of Galerkin’s
finite element method. For further details we refer the reader to the <a href="spde_nongaussian.html">Sampling from processes given by solutions
of SPDEs driven by non-Gaussian noise</a> vignette.</p>
<p>Recall that we are assuming <span class="math inline">\(\beta=1\)</span>, so our SPDE is given by <span class="math display">\[(\kappa^2 - \Delta) X(\mathbf{s}) =
\dot{\mathcal{M}}(\mathbf{s}),\quad
\mathbf{s}\in\mathcal{D}.\]</span></p>
<p>Let us introduce some notation regarding the finite element method
(FEM). Let <span class="math inline">\(V_n = {\rm
span}\{\varphi_1,\ldots,\varphi_n\}\)</span>, where <span class="math inline">\(\varphi_i(\mathbf{s}), i=1,\ldots, n\)</span> are
piecewise linear basis functions obtained from a triangulation of <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>An approximate solution <span class="math inline">\(X_n\)</span> of
<span class="math inline">\(X\)</span> is written in terms of the finite
element basis functions as <span class="math display">\[X_n(\mathbf{s})
= \sum_{i=1}^n w_i \varphi_i(\mathbf{s}),\]</span> where <span class="math inline">\(w_i\)</span> are the FEM weights. Let, also, <span class="math display">\[\mathbf{f} =
(\dot{\mathcal{M}}(\varphi_1),\ldots,
\dot{\mathcal{M}}(\varphi_n)),\]</span> Therefore, given a vector <span class="math inline">\(\mathbf{V} = (V_1,\ldots,V_n)\)</span> of
independent stochastic variances (in our case, positive infinitely
divisible random variables), we obtain that <span class="math display">\[\mathbf{f}|\mathbf{V} \sim N(\gamma +
\mu\mathbf{V}, \sigma^2{\rm diag}(\mathbf{V})).\]</span></p>
<p>Let us now introduce some useful notation. Let <span class="math inline">\(\mathbf{C}\)</span> be the <span class="math inline">\(n\times n\)</span> matrix with <span class="math inline">\((i,j)\)</span>th entry given by <span class="math display">\[\mathbf{C}_{i,j} = \int_{\mathcal{D}}
\varphi_i(\mathbf{s})\varphi_j(\mathbf{s}) d\mathbf{s}.\]</span> The
matrix <span class="math inline">\(\mathbf{C}\)</span> is known as the
<em>mass matrix</em> in FEM theory. Let, also, <span class="math inline">\(\mathbf{G}\)</span> be the <span class="math inline">\(n\times n\)</span> matrix with <span class="math inline">\((i,j)\)</span>th entry given by <span class="math display">\[\mathbf{G}_{i,j} = \int_{\mathcal{D}} \nabla
\varphi_i(\mathbf{s})\nabla\varphi_j(\mathbf{s})d\mathbf{s}.\]</span>
The matrix <span class="math inline">\(\mathbf{G}\)</span> is known in
FEM theory as stiffness matrix. Finally, let <span class="math display">\[h_i = \int_{\mathcal{D}} \varphi_i(\mathbf{s})
d\mathbf{s}, \quad i=1,\ldots,n.\]</span> Recall that <span class="math inline">\(\gamma = -\mu E(V)\)</span>. <span class="math inline">\(\mathbf{V}\)</span> is chosen such that <span class="math inline">\(E[V_i] = h_i\)</span> to ensure parameter
identifiability. Then, we have that the FEM weights <span class="math inline">\(\mathbf{w} = (w_1,\ldots,w_n)\)</span> satisfy
<span class="math display">\[\mathbf{w}|\mathbf{V} \sim
N(\mathbf{K}^{-1}(-\mu \mathbf{h}+\mu\mathbf{V}),
\sigma^2\mathbf{K}^{-1}{\rm diag}(\mathbf{V})\mathbf{K}^{-1}),\]</span>
where <span class="math inline">\(\mathbf{K} =
\kappa^2\mathbf{C}+\mathbf{G}\)</span> is the discretization of the
differential operator and <span class="math inline">\(\mathbf{h} =
(h_1,\ldots,h_n)\)</span>.</p>
</div>
<div class="section level2">
<h2 id="prediction">Prediction<a class="anchor" aria-label="anchor" href="#prediction"></a>
</h2>
<p>In order to illustrate predictions we will assume that the parameters
<span class="math inline">\(\kappa\)</span>, <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\sigma_\varepsilon\)</span> are known. The case of
unknown parameters will be treated in the next section.</p>
<p>Our goal in this section is to perform prediction of the latent field
<span class="math inline">\(X\)</span> at locations where there are no
observations. Usually, when doing such predictions, one provides mean
and variance of the predictive distribution. We will now describe how
one can generate these quantities when assuming the model defined in the
previous sections.</p>
<p>Let us assume we want to obtain predictions at locations <span class="math inline">\(\widetilde{\mathbf{s}}_1, \ldots,
\widetilde{\mathbf{s}}_p \in \mathcal{D}\)</span>, where <span class="math inline">\(p\in \mathbb{N}\)</span>.</p>
<p>Notice that for <span class="math inline">\(j=1,\ldots,p\)</span>,
<span class="math display">\[X_n(\widetilde{\mathbf{s}}_j) =
\sum_{i=1}^n w_i \varphi_i(\widetilde{\mathbf{s}}_j).\]</span>
Therefore, if we let <span class="math inline">\(\mathbf{A}_p\)</span>
be the <span class="math inline">\(p\times n\)</span> matrix whose <span class="math inline">\((i,j)\)</span>th entry is given by <span class="math inline">\(\mathbf{A}_{p,ij} =
\varphi_j(\widetilde{\mathbf{s}}_i)\)</span>, then <span class="math display">\[(X_n(\widetilde{\mathbf{s}}_1),\ldots,
X_n(\widetilde{\mathbf{s}}_p)) = \mathbf{A}_p\mathbf{w}.\]</span> Thus,
to perform prediction the desired means and variances are <span class="math display">\[E[\mathbf{A}_p \mathbf{w}  |
\mathbf{Y}]\quad\hbox{and}\quad
V[\mathbf{A}_p\mathbf{w}|\mathbf{Y}],\]</span> where <span class="math inline">\(\mathbf{Y} = (Y_1,\ldots,Y_N).\)</span></p>
<p>Now, observe that the density of <span class="math inline">\(\mathbf{w}|\mathbf{Y}\)</span> is not known. So,
the mean and variance cannot be computed analytically.</p>
<p>There are two ways to circumvent that situation. Both of them are
based on the fact that even though we do not know the density of <span class="math inline">\(\mathbf{w}|\mathbf{Y}\)</span>, we do know the
density of <span class="math inline">\(\mathbf{V}|\mathbf{w},\mathbf{Y}\)</span> and the
density of <span class="math inline">\(\mathbf{w}|\mathbf{V},\mathbf{Y}\)</span>.
Therefore we can use a Gibbs sampler to sample from <span class="math inline">\((\mathbf{w},\mathbf{V})|\mathbf{Y}\)</span>. From
this we obtain, as a byproduct, marginal samples from <span class="math inline">\(\mathbf{w}|\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{V}|\mathbf{Y}\)</span>.</p>
<p>We will now provide a brief presentation of the Gibbs sampler and
then we will provide the approximations of the means and variances.</p>
<div class="section level3">
<h3 id="gibbs-sampler">Gibbs sampler<a class="anchor" aria-label="anchor" href="#gibbs-sampler"></a>
</h3>
<p>We will now briefly describe the Gibbs sampler algorithm that will be
used here.</p>
<p>Let <span class="math inline">\(\mathbf{A}\)</span> be the <span class="math inline">\(N\times n\)</span> matrix, whose <span class="math inline">\((i,j)\)</span>th entry is given by <span class="math inline">\(\mathbf{A}_{ij} =
\varphi_j(\mathbf{s}_i)\)</span>. Therefore, we have that <span class="math display">\[(X_n(\mathbf{s}_1),\ldots,X_n(\mathbf{s}_N)) =
\mathbf{A}\mathbf{w},\]</span> so that <span class="math display">\[\mathbf{Y} \approx \mathbf{A}\mathbf{w} +
\boldsymbol{\varepsilon},\]</span> where <span class="math inline">\(\boldsymbol{\varepsilon} =
(\varepsilon_1,\ldots,\varepsilon_N)\)</span>. We will consider the
above representation, i.e., we will assume that <span class="math display">\[\mathbf{Y} = \mathbf{A}\mathbf{w} +
\boldsymbol{\varepsilon},\]</span> and that any error from the
approximation of <span class="math inline">\(X(\cdot)\)</span> by <span class="math inline">\(X_n(\cdot)\)</span> is captured by the measurement
noise.</p>
<p>Therefore, under this assumption we have that <span class="math display">\[\mathbf{Y}|\mathbf{w} \sim
N(\mathbf{A}\mathbf{w}, \sigma_\varepsilon^{2} \mathbf{I}).\]</span>
Also recall that <span class="math display">\[\mathbf{w}|\mathbf{V} \sim
N(\mathbf{K}^{-1}(-\mu \mathbf{h}+\mu\mathbf{V}),
\sigma^2\mathbf{K}^{-1}{\rm diag}(\mathbf{V})\mathbf{K}^{-1}).\]</span>
Let <span class="math display">\[\mathbf{m} = \mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V})\quad \hbox{and}\quad \mathbf{Q} =
\frac{1}{\sigma^2}\mathbf{K}{\rm
diag}(\mathbf{V})^{-1}\mathbf{K}.\]</span></p>
<p>It thus follows (see, also, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/sjos.12141" class="external-link">Wallin
and Bolin (2015)</a> or <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssc.12405?af=R" class="external-link">Asar
et al. (2020)</a>) that <span class="math display">\[\mathbf{w} |
\mathbf{V}, \mathbf{Y} \sim N\big(\widetilde{\mathbf{m}},
\widetilde{\mathbf{Q}}^{-1}),\]</span> where <span class="math display">\[\widetilde{\mathbf{Q}} = \mathbf{Q} +
\sigma_\varepsilon^{-2} \mathbf{A}^\top\mathbf{A}\quad\hbox{and}\quad
\widetilde{\mathbf{m}} =
\widetilde{\mathbf{Q}}^{-1}\big(\mathbf{Q}\mathbf{m}+\sigma_\varepsilon^{-2}\mathbf{A}^\top\mathbf{Y}\big).\]</span></p>
<p>To compute the conditional distribution <span class="math inline">\(\mathbf{V}|\mathbf{w}, \mathbf{Y}\)</span> one can
see from <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/sjos.12141" class="external-link">Wallin
and Bolin (2015)</a>, pp. 879, that <span class="math inline">\(V_1,\ldots,V_n\)</span> are conditionally
independent given <span class="math inline">\(\mathbf{w}\)</span>.
Furthermore, we also have from Proposition 1 from <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssc.12405?af=R" class="external-link">Asar
et al. (2020)</a>) that if <span class="math inline">\(V\sim
GIG(p,a,b)\)</span>, where <span class="math inline">\(GIG\)</span>
stands for the <a href="https://en.wikipedia.org/wiki/Generalized_inverse_Gaussian_distribution" class="external-link">generalized
inverse Gaussian distribution</a> with parameters <span class="math inline">\(p, a\)</span> and <span class="math inline">\(b\)</span>, then, for every <span class="math inline">\(j=1,\ldots,n\)</span>, <span class="math display">\[V_j|\mathbf{w},\mathbf{Y} \sim GIG\Bigg(p-0.5,
a+\frac{\mu^2}{\sigma^2}, b +
\frac{(\mathbf{K}\mathbf{w}+\mathbf{h}\mu)_j^2}{\sigma^2}\Bigg).\]</span></p>
<p>We are now in a position to use the Gibbs sampling algorithm:</p>
<ul>
<li>Provide initial values <span class="math inline">\(\mathbf{V}^{(0)}\)</span>;</li>
<li>Sample <span class="math inline">\(\mathbf{w}^{(1)} |
\mathbf{V}^{(0)},\mathbf{Y}\)</span>;</li>
<li>Sample <span class="math inline">\(\mathbf{V}^{(1)} |
\mathbf{w}^{(1)}, \mathbf{Y}\)</span>;</li>
<li>Continue by sequentially sampling <span class="math inline">\(\mathbf{w}^{(i)}|\mathbf{V}^{(i-1)},\mathbf{Y}\)</span>,
and then <span class="math inline">\(\mathbf{V}^{(i)}|\mathbf{w}^{(i)},
\mathbf{Y}\)</span> for <span class="math inline">\(i=1,\ldots,k\)</span>.</li>
</ul>
<p>One should stop when equilibrium is reached. To obtain evidence that
equilibrium has been achieved, it is best to consider more than one
chain, starting from different locations, and see if they mixed well. It
might also be useful to see autocorrelation plots.</p>
<p>Depending on the starting values, one might consider to do
<strong>burn-in</strong> samples, that is, one runs a chain for some
iterations, then saves the last position, throw away the rest of the
samples, and use that as starting values.</p>
<p>It is important to observe that the samples <span class="math inline">\(\{\mathbf{w}^{(i)},\mathbf{V}^{(i)}\}_{i=1}^k\)</span>
will not be independent. However, under very general assumptions, the
Gibbs sampler provides samples satisfying the law of large numbers for
functionals of the sample. Therefore, one can use these samples to
compute means and variances.</p>
</div>
<div class="section level3">
<h3 id="standard-mc-estimates">Standard MC estimates<a class="anchor" aria-label="anchor" href="#standard-mc-estimates"></a>
</h3>
<p>Suppose we have a sample <span class="math inline">\(\mathbf{w}^{(1)},\ldots, \mathbf{w}^{(k)}\)</span>
and then approximate the mean as <span class="math display">\[E[\mathbf{A}_p \mathbf{w}  | \mathbf{Y}] \approx
\frac{1}{k} \sum_{i=1}^k \mathbf{A}_p\mathbf{w}^{(i)}\]</span> and to
approximate the variance as <span class="math display">\[V[\mathbf{A}_p
\mathbf{w}|\mathbf{Y}] \approx \frac{1}{k} \sum_{i=1}^k
(\mathbf{A}_p\mathbf{w}^{(i)} -
E[\mathbf{A}_p\mathbf{w}|\mathbf{Y}])^2,\]</span> where <span class="math inline">\(\{\mathbf{w}^{(i)}\}_{i=1}^k\)</span> is a sample
generated using the Gibbs sampler algorithm.</p>
</div>
<div class="section level3">
<h3 id="rao-blackwellization-for-means-and-variances">Rao-Blackwellization for means and variances<a class="anchor" aria-label="anchor" href="#rao-blackwellization-for-means-and-variances"></a>
</h3>
<p>The second way consists in performing a Rao-Blackwellization (Robert
and Casella, 2004) for the means and variances. By following <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/sjos.12141" class="external-link">Wallin
and Bolin (2015)</a> we have that <span class="math display">\[
\begin{array}{ccl}
E[\mathbf{A}_p\mathbf{w}|\mathbf{Y}] &amp;=&amp; \int_\mathbf{w}
\mathbf{A}_p \mathbf{w} \pi(\mathbf{w}|\mathbf{Y})\,d\mathbf{w},\\
&amp;=&amp; \int_\mathbf{w} \int_{\mathbf{V}} \mathbf{A}_p \mathbf{w}
\pi(\mathbf{w}|\mathbf{V},\mathbf{Y})\pi(\mathbf{V}|\mathbf{Y})\,d\mathbf{V}\,d\mathbf{w}\\
&amp;=&amp; \int_\mathbf{V} \mathbf{A}_p \widetilde{\mathbf{m}}
\pi(\mathbf{V}|\mathbf{Y})\,d\mathbf{V},
\end{array}
\]</span> where <span class="math inline">\(\widetilde{\mathbf{m}} =
E[\mathbf{w}|\mathbf{V},\mathbf{Y}]\)</span>, and its expression was
given in the description of the Gibbs sampler.</p>
<p>Thus, we have the approximation <span class="math display">\[E[\mathbf{A}_p\mathbf{w}|\mathbf{Y}] \approx
\frac{1}{k}\sum_{i=1}^k \mathbf{A}_p
\widetilde{\mathbf{m}}^{(i)},\]</span> where <span class="math inline">\(\widetilde{\mathbf{m}}^{(i)} =
\widetilde{\mathbf{m}}(\mathbf{V}^{(i)})\)</span>, that is, <span class="math inline">\(\widetilde{\mathbf{m}}^{(i)}\)</span> was computed
based on <span class="math inline">\(\mathbf{V}^{(i)}\)</span>.</p>
<p>Notice, also, that <span class="math inline">\(\widetilde{\mathbf{m}}\)</span> has been computed
during the Gibbs sampling, so it does not imply on additional cost.</p>
<p>By a similar reasoning we have that <span class="math display">\[V(\mathbf{A}_p \mathbf{w}|\mathbf{Y}) \approx
\frac{1}{k}\sum_{i=1}^k\mathbf{A}_p^\top
(\widetilde{\mathbf{Q}}^{(i)})^{-1}\mathbf{A}_p,\]</span> where <span class="math inline">\(\widetilde{\mathbf{Q}}^{(i)}\)</span> is the
conditional covariance matrix of <span class="math inline">\(\mathbf{w}\)</span>, given <span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span>, evaluated at <span class="math inline">\(\mathbf{V}^{(i)}\)</span>.</p>
</div>
</div>
<div class="section level2">
<h2 id="examples-in-r">Examples in <em>R</em><a class="anchor" aria-label="anchor" href="#examples-in-r"></a>
</h2>
<div class="section level3">
<h3 id="spde-based-model-driven-by-nig-noise-with-gaussian-measurement-errors-in-1d">SPDE based model driven by NIG noise with Gaussian measurement
errors in 1D<a class="anchor" aria-label="anchor" href="#spde-based-model-driven-by-nig-noise-with-gaussian-measurement-errors-in-1d"></a>
</h3>
<p>For this example we will consider the latent process <span class="math inline">\(X\)</span> solving the equation <span class="math display">\[(\kappa^2 - \partial^2/\partial t^2)X(t) =
\dot{\mathcal{M}}(t),\]</span> where <span class="math inline">\(\dot{\mathcal{M}}\)</span> is a NIG-distributed
white-noise. For more details on the NIG distribution as well as on how
to sample from such a process we refer the reader to <a href="spde_nongaussian.html">Sampling from processes given by solutions
of SPDEs driven by non-Gaussian noise</a> vignette. We will also be
using the notation from that vignette.</p>
<p>Notice that we will, then, be assuming <span class="math inline">\(V_i\)</span> to follow an inverse Gaussian
distribution with parameters <span class="math inline">\(\nu\)</span>
and <span class="math inline">\(\nu h_i^2\)</span>.</p>
<p>We will take <span class="math inline">\(\kappa=1\)</span>, <span class="math inline">\(\sigma_\varepsilon=1\)</span>, <span class="math inline">\(\sigma=1\)</span>, <span class="math inline">\(\mu
= 1\)</span>, <span class="math inline">\(\mathcal{D}=[0,1]\)</span> and
<span class="math inline">\(\nu=0.5\)</span>. We will also assume that
we have 10 observations <span class="math display">\[Y_i = X(t_i) +
\varepsilon_i,\quad i=1,\ldots,10,\]</span> where <span class="math inline">\(t_1,\ldots,t_{10}\in [0,1]\)</span>. Notice that
<span class="math inline">\(\varepsilon_i \sim N(0,1)\)</span>.</p>
<p>Let us now build the matrix <span class="math inline">\(K\)</span>,
generate <span class="math inline">\(V\)</span> and then sample from the
NIG model:</p>
<p>We will consider a mesh with 10 equally spaced nodes. The code to
sample for such a process is given below.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://inlabru-org.github.io/fmesher/" class="external-link">fmesher</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidbolin.github.io/ngme2/">ngme2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyr.tidyverse.org" class="external-link">tidyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://Matrix.R-forge.R-project.org" class="external-link">Matrix</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="va">loc_1d</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="fl">9</span><span class="op">/</span><span class="fl">9</span></span>
<span><span class="va">mesh_1d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://inlabru-org.github.io/fmesher/reference/fm_mesh_1d.html" class="external-link">fm_mesh_1d</a></span><span class="op">(</span><span class="va">loc_1d</span><span class="op">)</span></span>
<span><span class="co"># inla.mesh.fem</span></span>
<span><span class="co"># inla.mesh.1d.fem(mesh_1d)$c1 + inla.mesh.1d.fem(mesh_1d)$g1</span></span>
<span><span class="co"># attr(simulate(noise_nig(n=10, 1,1,0.5), seed=10), "noise")$h</span></span>
<span></span>
<span><span class="co"># specify the model we use</span></span>
<span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fl">1</span>; <span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">1</span>; <span class="va">nu</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span><span class="va">spde_1d</span> <span class="op">&lt;-</span> <span class="fu">ngme2</span><span class="fu">::</span><span class="fu"><a href="../reference/f.html">f</a></span><span class="op">(</span></span>
<span>  <span class="va">loc_1d</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"matern"</span>,</span>
<span>  theta_K <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>,</span>
<span>  mesh <span class="op">=</span> <span class="va">mesh_1d</span>,</span>
<span>  noise <span class="op">=</span> <span class="fu"><a href="../reference/ngme_noise.html">noise_nig</a></span><span class="op">(</span>mu<span class="op">=</span><span class="va">mu</span>, sigma<span class="op">=</span><span class="va">sigma</span>, nu<span class="op">=</span><span class="va">nu</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">K</span> <span class="op">&lt;-</span> <span class="va">spde_1d</span><span class="op">$</span><span class="va">operator</span><span class="op">$</span><span class="va">K</span></span>
<span></span>
<span><span class="va">W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/simulate.html" class="external-link">simulate</a></span><span class="op">(</span><span class="va">spde_1d</span>, seed <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">loc_1d</span>, <span class="va">W</span>, type <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></span></code></pre></div>
<p><img src="pred_and_est_files/figure-html/generate-VW-1.png" width="700" style="display: block; margin: auto;"></p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">W</span><span class="op">)</span></span>
<span><span class="co">#&gt;  num [1:10] 0.00269 0.02077 0.03219 0.04505 0.01776 ...</span></span>
<span></span>
<span><span class="va">V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">W</span>, <span class="st">"noise"</span><span class="op">)</span><span class="op">$</span><span class="va">V</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">loc_1d</span>, <span class="va">V</span>, type <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></span></code></pre></div>
<p><img src="pred_and_est_files/figure-html/generate-VW-2.png" width="700" style="display: block; margin: auto;"></p>
<p>We will now generate 9 uniformly distributed random locations to
determine the locations of the observations. Once we determine the
locations we will need to build the <span class="math inline">\(A\)</span> matrix to obtain the values of the
process <span class="math inline">\(X\)</span> at those locations. We
can build the <span class="math inline">\(A\)</span> matrix by using the
function <em>fm_basis</em>.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_loc_1d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sort.html" class="external-link">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="fl">9</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">A_matrix</span> <span class="op">&lt;-</span> <span class="fu">fmesher</span><span class="fu">::</span><span class="fu"><a href="https://inlabru-org.github.io/fmesher/reference/fm_basis.html" class="external-link">fm_basis</a></span><span class="op">(</span></span>
<span>  mesh <span class="op">=</span> <span class="fu"><a href="https://inlabru-org.github.io/fmesher/reference/fm_mesh_1d.html" class="external-link">fm_mesh_1d</a></span><span class="op">(</span><span class="va">loc_1d</span><span class="op">)</span>,</span>
<span>  loc <span class="op">=</span> <span class="va">new_loc_1d</span></span>
<span><span class="op">)</span></span>
<span><span class="va">A_matrix</span></span>
<span><span class="co">#&gt; 9 x 10 sparse Matrix of class "dgCMatrix"</span></span>
<span><span class="co">#&gt;                                                                  </span></span>
<span><span class="co">#&gt;  [1,] . . . 0.8416668 0.1583332 .         .         .         . .</span></span>
<span><span class="co">#&gt;  [2,] . . . 0.5365185 0.4634815 .         .         .         . .</span></span>
<span><span class="co">#&gt;  [3,] . . . 0.2234306 0.7765694 .         .         .         . .</span></span>
<span><span class="co">#&gt;  [4,] . . . .         0.2899858 0.7100142 .         .         . .</span></span>
<span><span class="co">#&gt;  [5,] . . . .         .         0.4407827 0.5592173 .         . .</span></span>
<span><span class="co">#&gt;  [6,] . . . .         .         0.4294092 0.5705908 .         . .</span></span>
<span><span class="co">#&gt;  [7,] . . . .         .         .         0.8148042 0.1851958 . .</span></span>
<span><span class="co">#&gt;  [8,] . . . .         .         .         0.7875746 0.2124254 . .</span></span>
<span><span class="co">#&gt;  [9,] . . . .         .         .         0.2507151 0.7492849 . .</span></span>
<span><span class="va">sigma_eps</span> <span class="op">=</span> <span class="fl">1</span></span>
<span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">A_matrix</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">W</span> <span class="op">+</span> <span class="va">sigma_eps</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">9</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">new_loc_1d</span>, <span class="va">Y</span>, type<span class="op">=</span><span class="st">"h"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html" class="external-link">abline</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<p><img src="pred_and_est_files/figure-html/unnamed-chunk-2-1.png" width="672"></p>
<p>Now, let us assume we want to obtain prediction for <span class="math inline">\(X\)</span> at the point <span class="math inline">\(t^\ast = 5\)</span>. To this end we will obtain
predictions using both methods, namely, the standard MC and the
Rao-Blackwellization.</p>
<p>For both of these methods we need Gibbs samples, so let us build
Gibbs samples. We will build 2 chains with different starting values and
no burn-in samples.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># First step - Starting values for V</span></span>
<span><span class="co"># Considering a sample of inv-Gaussian as starting values for both chains</span></span>
<span><span class="va">h_nig_1d</span> <span class="op">&lt;-</span> <span class="va">spde_1d</span><span class="op">$</span><span class="va">operator</span><span class="op">$</span><span class="va">h</span></span>
<span><span class="va">V_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu">ngme2</span><span class="fu">::</span><span class="fu"><a href="../reference/ig.html">rig</a></span><span class="op">(</span><span class="fl">10</span>, <span class="va">nu</span>, <span class="va">nu</span><span class="op">*</span><span class="va">h_nig_1d</span><span class="op">^</span><span class="fl">2</span>, seed <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">V_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu">ngme2</span><span class="fu">::</span><span class="fu"><a href="../reference/ig.html">rig</a></span><span class="op">(</span><span class="fl">10</span>, <span class="va">nu</span>, <span class="va">nu</span><span class="op">*</span><span class="va">h_nig_1d</span><span class="op">^</span><span class="fl">2</span>, seed <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>Recall that <span class="math inline">\(V_j \sim IG(\nu, h_j^2 \cdot
\nu), j=1,\ldots,10\)</span>, <span class="math inline">\(\mu=1\)</span>
and <span class="math inline">\(\sigma=1\)</span>, so that <span class="math display">\[V_j|\mathbf{w},\mathbf{Y} \sim GIG(-1,\nu + 1,
\nu\cdot h_j^2 + (\mathbf{K}\mathbf{w}+\mathbf{h})_j^2)\]</span> and
<span class="math display">\[\mathbf{w} | \mathbf{V}, \mathbf{Y} \sim
N\big( (\mathbf{K}{\rm diag}(\mathbf{V})^{-1}\mathbf{K} +
\mathbf{A}^\top\mathbf{A})^{-1}(\mathbf{K}{\rm
diag}(\mathbf{V})^{-1}(\mathbf{V}-\mathbf{h})+\mathbf{A}^\top
\mathbf{Y}), (\mathbf{K}{\rm diag}(\mathbf{V})^{-1}\mathbf{K} +
\mathbf{A}^\top\mathbf{A})^{-1}\big).\]</span></p>
<p>Let us sample 1000 times for each chain. Let us also record the
values of <span class="math inline">\(E[\mathbf{w} | \mathbf{V},
\mathbf{Y}]\)</span> during the sampling.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">W_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fl">0</span>, nrow<span class="op">=</span><span class="fl">1</span>, ncol<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">W_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fl">0</span>, nrow<span class="op">=</span><span class="fl">1</span>, ncol<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">N_sim</span> <span class="op">=</span> <span class="fl">1000</span></span>
<span></span>
<span><span class="co"># Vector of conditional means E[w|V,Y]</span></span>
<span><span class="va">m_W_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fl">0</span>, nrow<span class="op">=</span><span class="fl">1</span>, ncol<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">m_W_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fl">0</span>, nrow<span class="op">=</span><span class="fl">1</span>, ncol<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Recall that sigma_eps = 1</span></span>
<span></span>
<span><span class="va">Asq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">A_matrix</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">A_matrix</span> <span class="op">/</span> <span class="va">sigma_eps</span><span class="op">^</span><span class="fl">2</span></span>
<span></span>
<span><span class="co"># Recall that mu = 1 and sigma = 1</span></span>
<span><span class="co"># Gibbs sampling</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N_sim</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">Q_1</span> <span class="op">&lt;-</span> <span class="va">K</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="va">V_1</span><span class="op">[</span><span class="va">i</span>,<span class="op">]</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">K</span><span class="op">/</span><span class="va">sigma</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="va">Q_2</span> <span class="op">&lt;-</span> <span class="va">K</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="va">V_2</span><span class="op">[</span><span class="va">i</span>,<span class="op">]</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">K</span><span class="op">/</span><span class="va">sigma</span><span class="op">^</span><span class="fl">2</span></span>
<span></span>
<span>  <span class="va">resp_1</span> <span class="op">&lt;-</span> <span class="va">Q_1</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">K</span>,<span class="op">(</span><span class="op">-</span><span class="va">h_nig_1d</span> <span class="op">+</span> <span class="va">V_1</span><span class="op">[</span><span class="va">i</span>,<span class="op">]</span><span class="op">)</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">A_matrix</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">Y</span><span class="op">/</span><span class="va">sigma_eps</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="va">resp_2</span> <span class="op">&lt;-</span> <span class="va">Q_2</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">K</span>,<span class="op">(</span><span class="op">-</span><span class="va">h_nig_1d</span> <span class="op">+</span> <span class="va">V_2</span><span class="op">[</span><span class="va">i</span>,<span class="op">]</span><span class="op">)</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">A_matrix</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">Y</span><span class="op">/</span><span class="va">sigma_eps</span><span class="op">^</span><span class="fl">2</span></span>
<span></span>
<span>  <span class="va">m_W_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">m_W_1</span>, <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">Q_1</span> <span class="op">+</span> <span class="va">Asq</span>, <span class="va">resp_1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">m_W_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">m_W_2</span>, <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">Q_2</span> <span class="op">+</span> <span class="va">Asq</span>, <span class="va">resp_2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>  <span class="va">Chol_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/chol.html" class="external-link">chol</a></span><span class="op">(</span><span class="va">Q_1</span> <span class="op">+</span> <span class="va">Asq</span><span class="op">)</span></span>
<span>  <span class="va">Chol_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/chol.html" class="external-link">chol</a></span><span class="op">(</span><span class="va">Q_2</span> <span class="op">+</span> <span class="va">Asq</span><span class="op">)</span></span>
<span></span>
<span>  <span class="va">W_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">W_1</span>, <span class="va">m_W_1</span><span class="op">[</span><span class="va">i</span><span class="op">+</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">Chol_1</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">W_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">W_2</span>, <span class="va">m_W_2</span><span class="op">[</span><span class="va">i</span><span class="op">+</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">Chol_2</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">V_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">V_1</span>, <span class="fu">ngme2</span><span class="fu">::</span><span class="fu"><a href="../reference/gig.html">rgig</a></span><span class="op">(</span><span class="fl">10</span>,</span>
<span>                           <span class="op">-</span><span class="fl">1</span>,</span>
<span>                           <span class="va">nu</span> <span class="op">+</span> <span class="op">(</span><span class="va">mu</span><span class="op">/</span><span class="va">sigma</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>,</span>
<span>                           <span class="va">nu</span><span class="op">*</span><span class="va">h_nig_1d</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="op">(</span><span class="va">K</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">W_1</span><span class="op">[</span><span class="va">i</span><span class="op">+</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">+</span><span class="va">h_nig_1d</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">/</span><span class="va">sigma</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">V_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">V_2</span>, <span class="fu">ngme2</span><span class="fu">::</span><span class="fu"><a href="../reference/gig.html">rgig</a></span><span class="op">(</span><span class="fl">10</span>,</span>
<span>                                <span class="op">-</span><span class="fl">1</span>,</span>
<span>                                <span class="va">nu</span> <span class="op">+</span> <span class="op">(</span><span class="va">mu</span><span class="op">/</span><span class="va">sigma</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>,</span>
<span>                                <span class="va">nu</span><span class="op">*</span><span class="va">h_nig_1d</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="op">(</span><span class="va">K</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">W_2</span><span class="op">[</span><span class="va">i</span><span class="op">+</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">+</span><span class="va">h_nig_1d</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">/</span><span class="va">sigma</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Let us organize the data to build traceplots.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df_V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>V <span class="op">=</span> <span class="va">V_1</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, chain <span class="op">=</span> <span class="st">"1"</span>, coord <span class="op">=</span> <span class="fl">1</span>, idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">N_sim</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">temp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>V <span class="op">=</span> <span class="va">V_2</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, chain <span class="op">=</span> <span class="st">"2"</span>, coord <span class="op">=</span> <span class="fl">1</span>, idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">N_sim</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">df_V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">df_V</span>, <span class="va">temp</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">temp_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>V <span class="op">=</span> <span class="va">V_1</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>, chain <span class="op">=</span> <span class="st">"1"</span>, coord <span class="op">=</span> <span class="va">i</span>, idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">N_sim</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">temp_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>V <span class="op">=</span> <span class="va">V_2</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>, chain <span class="op">=</span> <span class="st">"2"</span>, coord <span class="op">=</span> <span class="va">i</span>, idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">N_sim</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">df_V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">df_V</span>, <span class="va">temp_1</span>, <span class="va">temp_2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">df_W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>W <span class="op">=</span> <span class="va">W_1</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, chain <span class="op">=</span> <span class="st">"1"</span>, coord <span class="op">=</span> <span class="fl">1</span>, idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">N_sim</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">temp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>W <span class="op">=</span> <span class="va">W_2</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, chain <span class="op">=</span> <span class="st">"2"</span>, coord <span class="op">=</span> <span class="fl">1</span>, idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">N_sim</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">df_W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">df_W</span>, <span class="va">temp</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">temp_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>W <span class="op">=</span> <span class="va">W_1</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>, chain <span class="op">=</span> <span class="st">"1"</span>, coord <span class="op">=</span> <span class="va">i</span>, idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">N_sim</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">temp_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>W <span class="op">=</span> <span class="va">W_2</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>, chain <span class="op">=</span> <span class="st">"2"</span>, coord <span class="op">=</span> <span class="va">i</span>, idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">N_sim</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">df_W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">df_W</span>, <span class="va">temp_1</span>, <span class="va">temp_2</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Let us compare the posterior means:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">V</span></span>
<span><span class="co">#&gt; NULL</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/colSums.html" class="external-link">colMeans</a></span><span class="op">(</span><span class="va">V_1</span><span class="op">)</span></span>
<span><span class="co">#&gt;  [1] 0.02867265 0.06101310 0.06768444 0.05076801 0.05678794 0.07455169</span></span>
<span><span class="co">#&gt;  [7] 0.06281583 0.06359449 0.08668888 0.04962432</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/colSums.html" class="external-link">colMeans</a></span><span class="op">(</span><span class="va">V_2</span><span class="op">)</span></span>
<span><span class="co">#&gt;  [1] 0.03664791 0.05548684 0.06701723 0.05467433 0.07813323 0.06536818</span></span>
<span><span class="co">#&gt;  [7] 0.06512153 0.08114957 0.07960875 0.03353547</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="va">W</span><span class="op">)</span></span>
<span><span class="co">#&gt;  [1]  0.002694590  0.020771634  0.032185870  0.045046367  0.017760541</span></span>
<span><span class="co">#&gt;  [6]  0.002791359 -0.015611763 -0.021791327 -0.039828396 -0.044018875</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/colSums.html" class="external-link">colMeans</a></span><span class="op">(</span><span class="va">W_1</span><span class="op">)</span></span>
<span><span class="co">#&gt;  [1] -0.3417648 -0.3409139 -0.3381107 -0.3343951 -0.3280072 -0.3194661</span></span>
<span><span class="co">#&gt;  [7] -0.3131416 -0.3073292 -0.3019353 -0.3005066</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/colSums.html" class="external-link">colMeans</a></span><span class="op">(</span><span class="va">W_2</span><span class="op">)</span></span>
<span><span class="co">#&gt;  [1] -0.3594827 -0.3590511 -0.3555535 -0.3515848 -0.3453499 -0.3394409</span></span>
<span><span class="co">#&gt;  [7] -0.3340322 -0.3290037 -0.3274828 -0.3270751</span></span></code></pre></div>
<p>Let us begin by building traceplots for <span class="math inline">\(V\)</span>:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">df_V</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">idx</span>, y <span class="op">=</span> <span class="va">V</span>, col <span class="op">=</span> <span class="va">chain</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html" class="external-link">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html" class="external-link">facet_wrap</a></span><span class="op">(</span><span class="op">~</span> <span class="va">coord</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="pred_and_est_files/figure-html/unnamed-chunk-7-1.png" width="672">
and for <span class="math inline">\(W\)</span>:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">df_W</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">idx</span>, y <span class="op">=</span> <span class="va">W</span>, col <span class="op">=</span> <span class="va">chain</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html" class="external-link">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html" class="external-link">facet_wrap</a></span><span class="op">(</span><span class="op">~</span> <span class="va">coord</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="pred_and_est_files/figure-html/unnamed-chunk-8-1.png" width="672"></p>
<p>The traceplots appear to be healthy, not being stuck anywhere and the
chain apparently mixed well.</p>
<p>We can also build autocorrelation plots. To such an end let us
prepare the data frames.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">acf_V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html" class="external-link">acf</a></span><span class="op">(</span><span class="va">V_1</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, plot<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">$</span><span class="va">acf</span><span class="op">)</span></span>
<span><span class="va">df_V_acf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>acf <span class="op">=</span> <span class="va">acf_V</span>,</span>
<span>                       chain <span class="op">=</span> <span class="st">"1"</span>, coord <span class="op">=</span> <span class="fl">1</span>, lag <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">acf_V</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">acf_V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html" class="external-link">acf</a></span><span class="op">(</span><span class="va">V_2</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, plot<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">$</span><span class="va">acf</span><span class="op">)</span></span>
<span><span class="va">temp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>acf <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html" class="external-link">acf</a></span><span class="op">(</span><span class="va">V_2</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, plot<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">$</span><span class="va">acf</span><span class="op">)</span>,</span>
<span>                   chain <span class="op">=</span> <span class="st">"2"</span>, coord <span class="op">=</span> <span class="fl">1</span>, lag <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">acf_V</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">df_V_acf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">df_V_acf</span>, <span class="va">temp</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">acf_V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html" class="external-link">acf</a></span><span class="op">(</span><span class="va">V_1</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>, plot<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">$</span><span class="va">acf</span><span class="op">)</span></span>
<span>  <span class="va">temp_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>acf <span class="op">=</span> <span class="va">acf_V</span>,</span>
<span>                       chain <span class="op">=</span> <span class="st">"1"</span>, coord <span class="op">=</span> <span class="va">i</span>, lag <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">acf_V</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">acf_V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html" class="external-link">acf</a></span><span class="op">(</span><span class="va">V_2</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>, plot<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">$</span><span class="va">acf</span><span class="op">)</span></span>
<span>  <span class="va">temp_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>acf <span class="op">=</span> <span class="va">acf_V</span>,</span>
<span>                       chain <span class="op">=</span> <span class="st">"2"</span>, coord <span class="op">=</span> <span class="va">i</span>, lag <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">acf_V</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">df_V_acf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">df_V_acf</span>, <span class="va">temp_1</span>, <span class="va">temp_2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">acf_W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html" class="external-link">acf</a></span><span class="op">(</span><span class="va">W_1</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, plot<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">$</span><span class="va">acf</span><span class="op">)</span></span>
<span><span class="va">df_W_acf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>acf <span class="op">=</span> <span class="va">acf_W</span>,</span>
<span>                       chain <span class="op">=</span> <span class="st">"1"</span>, coord <span class="op">=</span> <span class="fl">1</span>, lag <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">acf_W</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">acf_W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html" class="external-link">acf</a></span><span class="op">(</span><span class="va">W_2</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, plot<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">$</span><span class="va">acf</span><span class="op">)</span></span>
<span><span class="va">temp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>acf <span class="op">=</span> <span class="va">acf_W</span>, chain <span class="op">=</span> <span class="st">"2"</span>,</span>
<span>                   coord <span class="op">=</span> <span class="fl">1</span>, lag <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">acf_W</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">df_W_acf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">df_W_acf</span>, <span class="va">temp</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">acf_W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html" class="external-link">acf</a></span><span class="op">(</span><span class="va">W_1</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>, plot<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">$</span><span class="va">acf</span><span class="op">)</span></span>
<span>  <span class="va">temp_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>acf <span class="op">=</span> <span class="va">acf_W</span>,</span>
<span>                       chain <span class="op">=</span> <span class="st">"1"</span>, coord <span class="op">=</span> <span class="va">i</span>, lag <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">acf_W</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">acf_W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html" class="external-link">acf</a></span><span class="op">(</span><span class="va">W_2</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>, plot<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">$</span><span class="va">acf</span><span class="op">)</span></span>
<span>  <span class="va">temp_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>acf <span class="op">=</span> <span class="va">acf_W</span>,</span>
<span>                       chain <span class="op">=</span> <span class="st">"2"</span>,</span>
<span>                   coord <span class="op">=</span> <span class="va">i</span>, lag <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">acf_W</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">df_W_acf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">df_W_acf</span>, <span class="va">temp_1</span>, <span class="va">temp_2</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Now, let us plot the autocorrelation plots for <span class="math inline">\(V\)</span>:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">df_V_acf</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">lag</span>,y<span class="op">=</span><span class="va">acf</span>, col<span class="op">=</span><span class="va">chain</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>          <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html" class="external-link">geom_bar</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="st">"identity"</span>, position <span class="op">=</span> <span class="st">"identity"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">xlab</a></span><span class="op">(</span><span class="st">'Lag'</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">ylab</a></span><span class="op">(</span><span class="st">'ACF'</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html" class="external-link">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">coord</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="pred_and_est_files/figure-html/unnamed-chunk-10-1.png" width="672"></p>
<p>Now, the autocorrelation plots for <span class="math inline">\(W\)</span>:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">df_W_acf</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">lag</span>,y<span class="op">=</span><span class="va">acf</span>, col<span class="op">=</span><span class="va">chain</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>          <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html" class="external-link">geom_bar</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="st">"identity"</span>, position <span class="op">=</span> <span class="st">"identity"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">xlab</a></span><span class="op">(</span><span class="st">'Lag'</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">ylab</a></span><span class="op">(</span><span class="st">'ACF'</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html" class="external-link">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">coord</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="pred_and_est_files/figure-html/unnamed-chunk-11-1.png" width="672"></p>
<p>We can see that the correlation is very low.</p>
<p>We will now move forward to obtain the predictions at <span class="math inline">\(t^\ast = 5/9\)</span>. We will compute MC and
Rao-Blackwellization for the predictions.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Computing A matrix at t^\ast</span></span>
<span></span>
<span><span class="va">A_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://inlabru-org.github.io/fmesher/reference/fm_basis.html" class="external-link">fm_basis</a></span><span class="op">(</span>mesh <span class="op">=</span> <span class="va">mesh_1d</span>, loc <span class="op">=</span> <span class="fl">5</span><span class="op">/</span><span class="fl">9</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># MC_estimate:</span></span>
<span></span>
<span><span class="va">AW_1</span> <span class="op">&lt;-</span> <span class="va">A_pred</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">W_1</span><span class="op">)</span></span>
<span><span class="va">MC_pred_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html" class="external-link">cumsum</a></span><span class="op">(</span><span class="va">AW_1</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">AW_1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">RB_1</span> <span class="op">&lt;-</span> <span class="va">A_pred</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">m_W_1</span><span class="op">)</span></span>
<span><span class="va">RB_pred_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html" class="external-link">cumsum</a></span><span class="op">(</span><span class="va">RB_1</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">AW_1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">df_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">AW_1</span><span class="op">)</span>, MC <span class="op">=</span> <span class="va">MC_pred_1</span>,</span>
<span>                      RB <span class="op">=</span> <span class="va">RB_pred_1</span>, chain <span class="op">=</span> <span class="st">"1"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">AW_2</span> <span class="op">&lt;-</span> <span class="va">A_pred</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">W_2</span><span class="op">)</span></span>
<span><span class="va">MC_pred_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html" class="external-link">cumsum</a></span><span class="op">(</span><span class="va">AW_2</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">AW_2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">RB_2</span> <span class="op">&lt;-</span> <span class="va">A_pred</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">m_W_2</span><span class="op">)</span></span>
<span><span class="va">RB_pred_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html" class="external-link">cumsum</a></span><span class="op">(</span><span class="va">RB_2</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">AW_2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">temp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>idx <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">AW_2</span><span class="op">)</span>, MC <span class="op">=</span> <span class="va">MC_pred_2</span>,</span>
<span>                      RB <span class="op">=</span> <span class="va">RB_pred_2</span>, chain <span class="op">=</span> <span class="st">"2"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">df_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">df_pred</span>, <span class="va">temp</span><span class="op">)</span></span>
<span></span>
<span><span class="va">df_pred</span> <span class="op">&lt;-</span> <span class="fu">tidyr</span><span class="fu">::</span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html" class="external-link">pivot_longer</a></span><span class="op">(</span><span class="va">df_pred</span>,</span>
<span>     cols <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"MC"</span>, <span class="st">"RB"</span><span class="op">)</span>,</span>
<span>     names_to <span class="op">=</span> <span class="st">"Method"</span>,</span>
<span>     values_to <span class="op">=</span> <span class="st">"Prediction"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">df_pred</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">idx</span>, y <span class="op">=</span> <span class="va">Prediction</span>, col<span class="op">=</span><span class="va">Method</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html" class="external-link">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">chain</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html" class="external-link">geom_line</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p><img src="pred_and_est_files/figure-html/unnamed-chunk-12-1.png" width="672"></p>
<p>We notice that the Rao-Blackwellization approach converges much
faster than the standard MC approach.</p>
</div>
</div>
<div class="section level2">
<h2 id="model-estimation">Model estimation<a class="anchor" aria-label="anchor" href="#model-estimation"></a>
</h2>
<p>In <code>ngme2</code>, we employ the maximum likelihood estimation
and stochastic gradient descent method to estimate the parameters.</p>
<div class="section level3">
<h3 id="stochastic-gradient-descent-and-maximum-likelihood">Stochastic gradient descent and maximum likelihood<a class="anchor" aria-label="anchor" href="#stochastic-gradient-descent-and-maximum-likelihood"></a>
</h3>
<p>For maximum likelihood estimation the goal is to minimize <span class="math inline">\(f({\boldsymbol{\theta}}) =
-L({\boldsymbol{\theta}}; \mathbf{Y})\)</span>, where <span class="math inline">\(L\)</span> is the log-likelihood function of <span class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>For non-Gaussian models there is an additional complication that the
log-likelihood function <span class="math inline">\(L\)</span> is not
known in explicit form. A way to solve this problem is to use Fisher’s
identity (Fisher, 1925). See also Douc et al. (2014) for further
details.</p>
<p>Let <span class="math inline">\(\mathbf{U} = (U_1,\ldots,
U_n)\)</span> be a sequence of observed random variables with latent
variables <span class="math inline">\(\mathbf{Z} =
(Z_1,\ldots,Z_n)\)</span>, <span class="math inline">\(Z_i\)</span>
being a random variable in <span class="math inline">\(\mathbb{R}^p\)</span>. Assume that the joint
distribution of <span class="math inline">\(\mathbf{U}\)</span> and
<span class="math inline">\(\mathbf{Z}\)</span> is parameterized by some
<span class="math inline">\({\boldsymbol{\theta}}\)</span>, where <span class="math inline">\(\mathbf{{\boldsymbol{\theta}}} \in \Theta\)</span>
and <span class="math inline">\(\Theta\subset\mathbb{R}^p\)</span>.
Assume that the complete log-likelihood <span class="math inline">\(L({\boldsymbol{\theta}};
\mathbf{U},\mathbf{Z})\)</span> (with respect to some reference <span class="math inline">\(\sigma\)</span>-finite measure) is differentiable
with respect to <span class="math inline">\({\boldsymbol{\theta}}\)</span> and are regular, in
the sense that one may differentiate through the integral sign. Then,
the marginal log-likelihood with respect to <span class="math inline">\(\mathbf{U}\)</span> satisfies <span class="math display">\[\nabla_{\boldsymbol{\theta}}
L({\boldsymbol{\theta}}; \mathbf{U}) =
E_{\mathbf{Z}}[\nabla_{\boldsymbol{\theta}} L({\boldsymbol{\theta}};
\mathbf{U}, \mathbf{Z})|\mathbf{U}].\]</span></p>
</div>
<div class="section level3">
<h3 id="standard-mc-approximation-of-the-gradient">Standard MC approximation of the gradient<a class="anchor" aria-label="anchor" href="#standard-mc-approximation-of-the-gradient"></a>
</h3>
<p>In our context, we assume <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> to be hidden. Therefore, we
may use Fisher’s identity above to the latent variable <span class="math inline">\((\mathbf{V},\mathbf{w})\)</span> to obtain that
<span class="math display">\[\nabla_{\boldsymbol{\theta}}
L({\boldsymbol{\theta}}; \mathbf{Y}) =
E_{\mathbf{V},\mathbf{w}}[\nabla_{\boldsymbol{\theta}}
L({\boldsymbol{\theta}}; \mathbf{Y}, \mathbf{V},
\mathbf{w})|\mathbf{Y}].\]</span> Thus, the idea here is to use both
samples of <span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> obtained from the Gibbs
sampler to approximate the gradient as</p>
<p><span class="math display">\[\nabla_{{\boldsymbol{\theta}}}L({\boldsymbol{\theta}};\mathbf{Y})
\approx \frac{1}{k} \sum_{j=1}^k \nabla_{{\boldsymbol{\theta}}}
L({\boldsymbol{\theta}};\mathbf{Y},\mathbf{V}^{(j)},
\mathbf{w}^{(j)}).\]</span> To this end, we will compute the gradients
<span class="math inline">\(\nabla_{{\boldsymbol{\theta}}}
L({\boldsymbol{\theta}};\mathbf{Y},\mathbf{V}, \mathbf{w})\)</span>.</p>
<p>We have that <span class="math display">\[\mathbf{Y}|\mathbf{w} \sim
N(\mathbf{A}\mathbf{w}, \sigma_\varepsilon^{-2} \mathbf{I}),\]</span>
<span class="math display">\[\mathbf{w}|\mathbf{V} \sim
N(\mathbf{K}^{-1}(-\mu \mathbf{h}+\mu\mathbf{V}), \mathbf{K}^{-1}{\rm
diag}(\mathbf{V})\mathbf{K}^{-1})\]</span> and <span class="math inline">\(\mathbf{V}\)</span> follows a GIG distribution
such that for every <span class="math inline">\(i\)</span>, <span class="math inline">\(E[V_i]=h_i\)</span>.</p>
<p>Therefore, we have that <span class="math display">\[\begin{array}{ccl}
L((\mu,\sigma_\varepsilon); \mathbf{w}, \mathbf{V},\mathbf{Y})
&amp;=&amp; -n\log(\sigma_\varepsilon)-0.5\sigma_\varepsilon^{-2}
(\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V}))\\
&amp;-&amp;0.5\sigma_\varepsilon^{-2}(\mathbf{A}(\mathbf{w}-\mathbf{m})^\top{\rm
diag} (1/V_i) (\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V})-\mathbf{A}(\mathbf{w}-\mathbf{m})) + const,
\end{array}\]</span> where <span class="math inline">\(const\)</span>
does not depend on <span class="math inline">\((\mu,\sigma)\)</span>.
Thus, we have that <span class="math display">\[\nabla_\mu
L((\mu,\sigma_\varepsilon); \mathbf{w}, \mathbf{V},\mathbf{Y})
=  \sigma_\varepsilon^{-2}\mathbf{A}\mathbf{K}^{-1}(-\mathbf{h}+\mathbf{V})
{\rm diag} (1/V_i)(\mathbf{Y}  - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V}) - \mathbf{A}(\mathbf{w}-\mathbf{m})).\]</span>
Now, with respect to <span class="math inline">\(\sigma_\varepsilon\)</span> we have that <span class="math display">\[\nabla_{\sigma_\varepsilon}
L((\mu,\sigma_\varepsilon); \mathbf{w}, \mathbf{V},\mathbf{Y}) =
-\frac{n}{\sigma_\varepsilon} + \frac{1}{\sigma_\varepsilon^3}
(\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V})-\mathbf{A}(\mathbf{w}-\mathbf{m}))^\top{\rm
diag} (1/V_i) (\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V})-\mathbf{A}(\mathbf{w}-\mathbf{m}))\]</span> By
proceeding analogously, we obtain that the gradient with respect to
<span class="math inline">\(\kappa^2\)</span> is given by <span class="math display">\[\nabla_{\kappa^2} L(\kappa^2; \mathbf{Y},
\mathbf{w}, \mathbf{V}) = tr(\mathbf{C}\mathbf{K}^{-1})- \mathbf{w}^\top
\mathbf{C}^\top{\rm diag}
(1/V_i)(\mathbf{K}\mathbf{w}+(\mathbf{h}-\mathbf{V})\mu).\]</span>
Finally, for the gradient of the parameter of the distribution of <span class="math inline">\(\mathbf{V}\)</span>, we use the Rao-Blackwellized
version, see the next subsection.</p>
</div>
<div class="section level3">
<h3 id="rao-blackwellized-approximation-of-the-gradient">Rao-Blackwellized approximation of the gradient<a class="anchor" aria-label="anchor" href="#rao-blackwellized-approximation-of-the-gradient"></a>
</h3>
<p>Now, observe that we can compute the log-likelihood <span class="math inline">\(L({\boldsymbol{\theta}}; \mathbf{Y},
\mathbf{V})\)</span>. Indeed, we apply Fisher’s identity again to find
that <span class="math display">\[\nabla_{\boldsymbol{\theta}}
L({\boldsymbol{\theta}}; \mathbf{Y}, \mathbf{V}) =
E_\mathbf{w}[\nabla_{\boldsymbol{\theta}} L({\boldsymbol{\theta}};
\mathbf{Y}, \mathbf{V}, \mathbf{w})|\mathbf{Y},\mathbf{V}].\]</span> So,
with the above gradients, we can approximate the gradient <span class="math inline">\(\nabla_{\boldsymbol{\theta}}
L({\boldsymbol{\theta}}; \mathbf{Y})\)</span> by taking the mean over
the samples of <span class="math inline">\(\mathbf{V}\)</span> obtained
by the Gibbs sampling: <span class="math display">\[\nabla_{{\boldsymbol{\theta}}}L({\boldsymbol{\theta}};\mathbf{Y})
\approx \frac{1}{k} \sum_{j=1}^k \nabla_{{\boldsymbol{\theta}}}
L({\boldsymbol{\theta}};\mathbf{Y},\mathbf{V}^{(j)}).\]</span></p>
<p>Let us now compute the gradients <span class="math inline">\(\nabla_{{\boldsymbol{\theta}}}
L({\boldsymbol{\theta}};\mathbf{Y},\mathbf{V})\)</span>. We begin by
computing <span class="math inline">\(\nabla_\mu
L((\mu,\sigma_\varepsilon); \mathbf{V},\mathbf{Y})\)</span>. To this
end, we use the expression for <span class="math inline">\(\nabla_\mu
L((\mu,\sigma_\varepsilon); \mathbf{w}, \mathbf{V},\mathbf{Y})\)</span>
given in the previous subsection together with <span class="math inline">\(E[\mathbf{w}|\mathbf{V},\mathbf{Y}] =
\widetilde{\mathbf{m}}\)</span>, to conclude that <span class="math display">\[\nabla_\mu L((\mu,\sigma_\varepsilon);
\mathbf{V},\mathbf{Y})
=  \sigma_\varepsilon^{-2}\mathbf{A}\mathbf{K}^{-1}(-\mathbf{h}+\mathbf{V})
{\rm diag} (1/V_i)(\mathbf{Y}  - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V}) -
\mathbf{A}(\widetilde{\mathbf{m}}-\mathbf{m}))\]</span> Analogously, we
also obtain that <span class="math display">\[\nabla_{\sigma_\varepsilon}
L((\mu,\sigma_\varepsilon); \mathbf{V},\mathbf{Y}) =
-\frac{n}{\sigma_\varepsilon} + \frac{1}{\sigma_\varepsilon^3}
(\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V})-\mathbf{A}(\widetilde{\mathbf{m}}-\mathbf{m}))^\top{\rm
diag} (1/V_i) (\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu
\mathbf{h}+\mu\mathbf{V})-\mathbf{A}(\widetilde{\mathbf{m}}-\mathbf{m})).\]</span></p>
<p>Now, notice that <span class="math display">\[\nabla_{\kappa^2}
L(\kappa^2; \mathbf{Y}, \mathbf{w}, \mathbf{V}) =
tr(\mathbf{C}\mathbf{K}^{-1})- \mathbf{w}^\top \mathbf{C}^\top{\rm diag}
(1/V_i)\mathbf{K}\mathbf{w}-\mathbf{w}^\top \mathbf{C}^\top{\rm diag}
(1/V_i)(\mathbf{h}-\mathbf{V})\mu,\]</span> that <span class="math display">\[E[\mathbf{w}|\mathbf{V},\mathbf{Y}] =
\widetilde{\mathbf{m}}\]</span> and that <span class="math display">\[E[\mathbf{w}^\top \mathbf{C}^\top{\rm diag}
(1/V_i)\mathbf{K}\mathbf{w}|\mathbf{V},\mathbf{Y}]
=  tr(\mathbf{C}^\top{\rm diag}
(1/V_i)\mathbf{K}\widetilde{\mathbf{Q}}^{-1})
+   \widetilde{\mathbf{m}}^\top\mathbf{C}^\top{\rm diag}
(1/V_i)\mathbf{K}\widetilde{\mathbf{m}}\]</span> to conclude that <span class="math display">\[\nabla_{\kappa^2} L(\kappa^2; \mathbf{Y},
\mathbf{V}) = tr(\mathbf{C}\mathbf{K}^{-1})- tr(\mathbf{C}^\top{\rm
diag} (1/V_i)\mathbf{K}\widetilde{\mathbf{Q}}^{-1})
-   \widetilde{\mathbf{m}}^\top\mathbf{C}^\top{\rm diag}
(1/V_i)\mathbf{K}\widetilde{\mathbf{m}}-\widetilde{\mathbf{m}}^\top
\mathbf{C}^\top{\rm diag}
(1/V_i)(\mathbf{h}-\mathbf{V})\mu.\]</span></p>
<p>Finally, the gradient for the parameter of the distribution of <span class="math inline">\(\mathbf{V}\)</span> depends on the distribution of
<span class="math inline">\(\mathbf{V}\)</span>. To illustrate we will
present the gradient with respect to the parameter <span class="math inline">\(\nu\)</span> when <span class="math inline">\(\mathbf{V}\)</span> follows inverse-Gaussian
distribution, which is the situation in which we have NIG noise.</p>
<p>For this case we have that <span class="math display">\[\nabla_\nu
L(\nu; \mathbf{Y},\mathbf{V}) = -\sum_{j=1}^n \frac{1}{2}\Bigg(\nu^{-1}
-\frac{h_{j}^2}{V_j} +V_j -h_j\Bigg).\]</span></p>
<div class="section level4">
<h4 id="a-remark-on-traces">A remark on traces<a class="anchor" aria-label="anchor" href="#a-remark-on-traces"></a>
</h4>
<p>On the gradients <span class="math inline">\(\nabla_{\kappa^2}
L(\kappa^2; \mathbf{Y}, \mathbf{V})\)</span> and <span class="math inline">\(\nabla_{\kappa^2} L(\kappa^2; \mathbf{Y},
\mathbf{w}, \mathbf{V})\)</span>, we can see the traces <span class="math inline">\(tr(\mathbf{C}\mathbf{K}^{-1})\)</span> and <span class="math inline">\(tr(\mathbf{C}^\top{\rm diag}
(1/V_i)\mathbf{K}\widetilde{\mathbf{Q}}^{-1})\)</span>. These traces
contain the inverses <span class="math inline">\(\mathbf{K}^{-1}\)</span> and <span class="math inline">\(\widetilde{\mathbf{Q}}^{-1}\)</span>.</p>
<p>There are efficient alternatives to handling these traces. For
instance, if we want to compute <span class="math inline">\(tr(AB^{-1})\)</span>, <span class="math inline">\(B\)</span> is symmetric, and the sparsity of <span class="math inline">\(B\)</span> is the same as the sparsity of <span class="math inline">\(A\)</span>, we only need to compute the elements
of <span class="math inline">\(B^{-1}\)</span> for the coordinates with
non-zero entries. This is what happens, for instance, in <span class="math inline">\(tr(\mathbf{C}\mathbf{K}^{-1})\)</span>. So, to
compute this trace there is this efficient alternative. It is
implemented in the <em>ngme</em> package.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li><p>Lindgren, F., Rue, H., and Lindstrom, J. (2011). An explicit link
between Gaussian fields and Gaussian Markov random fields: the
stochastic partial differential equation approach. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
73(4):423–498.</p></li>
<li><p>Robert, C., G. Casella (2004). Monte Carlo statistical methods,
Springer Texts in Statistics, Springer, New York, USA.</p></li>
<li><p>Wallin, J., Bollin, D. (2015). Geostatistical Modelling Using
Non-Gaussian Matérn Fields. Scandinavian Journal of Statistics.
42(3):872-890.</p></li>
<li><p>Whittle, P. (1963). Stochastic-processes in several dimensions.
Bulletin of the International Statistical Institute,
40(2):974–994.</p></li>
</ul>
<!--
### Estimation

In ngme2, we use maximum likelihood estimation via preconditioned stochastic gradient descent.
The gradient is approximated by

#### Standard MC approximation of the gradient

In our context, we assume $\mathbf{w}$ and $\mathbf{V}$
to be hidden. Therefore, we may use Fisher's identity (Fisher, 1925) to the latent variable
$(\mathbf{V},\mathbf{w})$ to obtain that
$$\nabla_{\boldsymbol{\theta}} L({\boldsymbol{\theta}}; \mathbf{Y}) = E_{\mathbf{V},\mathbf{w}}[\nabla_{\boldsymbol{\theta}} L({\boldsymbol{\theta}}; \mathbf{Y}, \mathbf{V}, \mathbf{w})|\mathbf{Y}].$$

Thus, the idea here is to use both samples of $\mathbf{V}$ and $\mathbf{w}$ obtained from the Gibbs
sampler to approximate the gradient as
$$\nabla_{{\boldsymbol{\theta}}}L({\boldsymbol{\theta}};\mathbf{Y}) \approx \frac{1}{k} \sum_{j=1}^k \nabla_{{\boldsymbol{\theta}}} L({\boldsymbol{\theta}};\mathbf{Y},\mathbf{V}^{(j)}, \mathbf{w}^{(j)}).$$
To this end, we will compute the gradients $\nabla_{{\boldsymbol{\theta}}} L({\boldsymbol{\theta}};\mathbf{Y},\mathbf{V}, \mathbf{w})$.
We have that
$$\mathbf{Y}|\mathbf{w} \sim N(\mathbf{A}\mathbf{w}, \sigma_\varepsilon^{-2} \mathbf{I}),$$
$$\mathbf{w}|\mathbf{V} \sim N(\mathbf{K}^{-1}(-\mu \mathbf{h}+\mu\mathbf{V}), \sigma^2 \mathbf{K}^{-1}{\rm diag}(\mathbf{V})\mathbf{K}^{-1})$$
and $\mathbf{V}$ follows a GIG distribution such that for every $i$, $E[V_i]=h_i$.
Therefore, we have that
$$\begin{array}{ccl}
L((\mu,\sigma_\varepsilon); \mathbf{w}, \mathbf{V},\mathbf{Y}) &=& -n\log(\sigma_\varepsilon)-0.5\sigma_\varepsilon^{-2} (\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu \mathbf{h}+\mu\mathbf{V}))\\
&-&0.5\sigma_\varepsilon^{-2}(\mathbf{A}(\mathbf{w}-\mathbf{m})^\top{\rm diag} (1/(\sigma^2 V_i)) (\mathbf{Y} - \mathbf{A}\mathbf{K}^{-1}(-\mu \mathbf{h}+\mu\mathbf{V})-\mathbf{A}(\mathbf{w}-\mathbf{m})) + const,
\end{array}$$
where $const$ does not depend on $(\mu,\sigma)$. -->
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by David Bolin, Xiaotian Jin, Alexandre Simas, Jonas Wallin.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
