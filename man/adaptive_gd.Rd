% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizer.R
\name{adaptive_gd}
\alias{adaptive_gd}
\title{Adaptive gradient descent
From the paper: https://arxiv.org/pdf/1910.09529
The update rule for adaptive gradient descent is:
\deqn{\lambda_k = \min(\sqrt{1 + \theta_{k-1}} \lambda_{k-1}, \frac{||x_k - x_{k-1}||}{2 ||\nabla f(x_k) - \nabla f(x_{k-1})||} )}
\deqn{x_{k+1} = x_k - \lambda_k \nabla f(x_k)}
\deqn{\theta_k = \lambda_k / \lambda_{k-1}}}
\usage{
adaptive_gd(stepsize = 0.01)
}
\arguments{
\item{stepsize}{initial stepsize for SGD}
}
\value{
a list of control variables for optimization
(used in \code{control_opt} function)
}
\description{
Adaptive gradient descent
From the paper: https://arxiv.org/pdf/1910.09529
The update rule for adaptive gradient descent is:
\deqn{\lambda_k = \min(\sqrt{1 + \theta_{k-1}} \lambda_{k-1}, \frac{||x_k - x_{k-1}||}{2 ||\nabla f(x_k) - \nabla f(x_{k-1})||} )}
\deqn{x_{k+1} = x_k - \lambda_k \nabla f(x_k)}
\deqn{\theta_k = \lambda_k / \lambda_{k-1}}
}
